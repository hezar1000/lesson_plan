---
title: "Estimation of out-of-sample prediction error in regression"
---

## Assumptions about students
When designing this lesson plan, I've assumed that students

- have taken introductory courses in probability, calculus, and linear algebra. 
- have taken courses in statistical learning but have mostly focused on inference rather than prediction.  
- are comfortable writing code in Python and have used libraries such as `numpy`, `pandas`, `matplotlib`, and `sklearn` in previous courses (e.g., to fit a linear regression model) but have not used them for more advanced tasks such as cross-validation.
- are comfortable with running Python code in a jupyter lab environment on their own. 
- are familiar with the matrix notation used in statistical learning.


## Learning outcomes

By the end of this lesson, students should be able to:

- Identify the difference between **inference** and **prediction** in regression.
- Understand the **training/test/validation data split**.
- Define out-of-sample prediction error and **explain** its importance.
- Use Python to estimate out-of-sample prediction error via **cross-validation** given training data.
- Understand the **Bias-Variance Trade-Off** and its relationship to prediction error.


## Outline

### Lecture 1: Basics

- Regression definition
- Inference vs Prediction
- Training/test split
- Out-of-sample prediction error in regression
    - Definition
    - Why do we care about estimating out-of-sample error?
    - In-class discussion: Can we ever make a prediction with zero prediction error?
        - Goals:
            - Give students a chance to digest and reflect on the topics covered so far
            - Remind them of reducible and irreducible error
- Estimating the prediction error
    - Motivation
    - First approach: Estimating prediction error with in-sample (training) error
        - Example: Linear Regression
            - In-class live-coding activity: Calculate and compare the training and test errors using different seeds.
                - Goals:
                    - Give students a chance to play with training and test errors on a simple regression model
                    - Give them a chance to observe how different they can be
- The Bias-Variance Trade-Off
    - Motivating example: 10th order polynomial regression
    - Overfitting vs underfitting
    - Definition

### Lecture 2: Cross-validation

- Estimating the prediction error (continued)
    - Splitting the training data to get a validation set
    - Second approach: A single held-out point
        - In-class discussion: Discussion of the estimate for the out-of-sample prediction error via a single held-out point
            - Goals:
                - Give students a chance to digest the idea of a validation set
                - Make them more comfortable with estimating the prediction error with training data
    - Third approach: Cross-validation
        - LOOCV
        - K-fold cross-validation
    - Implementing cross-validation in Python
        - How to perform LOOCV in Python
        - How to perform $K$-fold cross-validation in Python
    - In-class live-coding activity: $K$-fold cross-validation
        - Goals:
            - Let students play with $K$-fold cross-validation and get comfortable with the idea
            - Get a sense for Bias-Variance Trade-Off for $K$-Fold cross-validation via an example
- Bias-Variance Trade-Off for K-fold cross-validation
