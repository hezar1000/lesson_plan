[
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "An Introduction to Statistical Learning with Applications in Python, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2023.\nLecture Notes on Statistical Prediction, Ryan Tibshirani, 2021.\nLecture notes on Error and Validation, Ryan Tibshirani, 2014.\nI also used ChatGPT for a little bit of brainstorming, proof-reading, and genereating code.",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "ref.html#references",
    "href": "ref.html#references",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "An Introduction to Statistical Learning with Applications in Python, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2023.\nLecture Notes on Statistical Prediction, Ryan Tibshirani, 2021.\nLecture notes on Error and Validation, Ryan Tibshirani, 2014.\nI also used ChatGPT for a little bit of brainstorming, proof-reading, and genereating code.",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "When designing this lesson plan, I’ve assumed that students\n\nhave taken introductory courses in probability, calculus, and linear algebra.\nhave taken courses in statistical learning but have mostly focused on inference rather than prediction.\n\nare comfortable writing code in Python and have used libraries such as numpy, pandas, matplotlib, and sklearn in previous courses (e.g., to fit a linear regression model) but have not used them for more advanced tasks such as cross-validation.\nare comfortable running Python code in a jupyter lab enviroment on their own.\nare familiar with the matrix notation used in statistical learning.",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "index.html#assumptions-about-students",
    "href": "index.html#assumptions-about-students",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "When designing this lesson plan, I’ve assumed that students\n\nhave taken introductory courses in probability, calculus, and linear algebra.\nhave taken courses in statistical learning but have mostly focused on inference rather than prediction.\n\nare comfortable writing code in Python and have used libraries such as numpy, pandas, matplotlib, and sklearn in previous courses (e.g., to fit a linear regression model) but have not used them for more advanced tasks such as cross-validation.\nare comfortable running Python code in a jupyter lab enviroment on their own.\nare familiar with the matrix notation used in statistical learning.",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of this lesson, students should be able to:\n\nIndentify the difference between inference and prediction in regression.\n\nUnderstand the training/test/validation data split\nDefine out-of-sample prediction error in theory and identify its importance.\n\nUse Python to estimate out-of-sample prediction error via cross-validation given training data .\n\nUnderstand the Bias-Variance Trade-Off and it relationship to prediction error.",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "Outline",
    "text": "Outline\n\nLecture 1: Basics\n\nRegression definition\nInference vs Prediction\nTraining/test split\nOut-of-sample prediction error in regression\n\nDefinition\nWhy do we care about estimating out-of-sample error?\nIn-class discussion: can we ever make a prediction with zero prediction error?\n\nGoals:\n\ngive students a chance to digest and reflect on the topics covers so far,\nremind them of reducible and irreducible error\n\n\n\nEstimating the prediction error\n\nMotivation\nFirst approach: estimating prediction error with in-sample (training) error\n\nExample: Linear Regression\n\nIn-class live-coding activity: calculate and compare the training and test errors using different seeds.\n\nGoals:\n\ngive students a chance to play with training and test errors on simple regression model\ngive thema chance to observe how different they can be.\n\n\n\n\n\nThe Bias-Variance Trade-Off\n\nMotivating example: 10th order polynomial regression\nDefinition\n\n\nLecture 2: Cross validation\n\nEstimating the prediction error (continued)\n\nSplitting the training data to get a validation set\nSecond approach: A single held-out point\n\nIn-class discussion: Discussion of estimate for the out-of-sample prediction error via a single held-out point\n\nGoals:\n\nGive students a chance to digest the idea of a validation set\nMake them more comfortable with estimating the prediction error with training data\n\n\n\nThird approach: Cross-validation\n\nLOOCV\nK-fold cross-calidation\n\nImplementing cross-valication in Python\n\nHow to perform LOOCV in Python\nHow to perform \\(K\\)-fold cross-validation in Python\n\n\nBias-Variance Trade-Off for k-Fold cross-validation",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "lect1.html",
    "href": "lect1.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lectures students should be able to:\n\nunderstand the difference between inference and prediction in regression (reminder)\nunderstand the training/test data split\nunderstand the difference between training (in-sample) and test (out-of-sample) prediction errors and define them\nunderstand the imporance of out-of-sample prediction error\nuse Python to estimate out-of-sample prediction error via (in-sample) training error in linear regression given the training data\nunderstand the draw back of estimating the out-of-sample prediction error via training error\nunderstand the Bias-Variance Trade-Off\n\n\n\n\n\nWe have \\(n\\) data points with \\(p\\) different predictors: \\(X = (X_1, X_2,...,X_p)\\) and a response \\(Y\\), which are independent samples from some distribution.\nWe assume that there is some relationship between which can be written as \\[Y = f(X) + \\epsilon,\\] where \\(f\\) is some fixed but unknown function of \\(X_1,...,X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero.\nOur goal is to estimate \\(f\\) via another function called \\(\\hat{f}\\).\nWhen \\(Y\\) is qualitative, we have a regression problem (and when it is qualitative, we have a classification problem).\n\n\n\n\n\n\n\nInference: Focuses on identifying relationships and associations between predictors and the response.\nPrediction: Focuses on estimating the response for newly unseen predictors.\n\nOur focus here is on prediction. But how do we know how good we are at this task?\n\n\n\n\nWhen it comes to prediction, we want to work with two different datasets: training and test (aka out-of-sample). The reason for this is that we would then consider a training phase, during which the training data is used to fit (or train) the model, and a testing phase, where the model’s performance is evaluated using the test dataset, which it has never seen during training. This evaluation helps us determine how well the model predicts responses for new, unseen data. From now on, we are going to use \\(X, Y\\) to denote the training data, and \\(X^*, Y^*\\) to denote the test data. We therefore can define the prediciton task more formally by defining it as estimating the regression function \\(\\hat{f}\\) as\n\\[\\hat{Y^*} = \\hat{f}(X^*)\\]\nfor an unseen \\(X^*\\), where \\(X^*, Y^*\\) are sampled from the same distribution as \\(X, Y\\)\n\n\n\nFor new test data \\(X^*, Y^*\\) sampled from the same distribution as the trainign data, we can defined the following error term called out-of-sample prediction error or prediction error in regression: \\[\\mathbb{E}(Y^*−\\hat{Y}^*)^2 = \\mathbb{E}(Y^*−\\hat{f}(X^*))^2,\\] where the expectation is taken over the training data \\(X, Y\\), and test data \\(X^*, Y^*\\).\n\n\n\nTo evalute the quality of predicition in our model,\nTo be able to minimize the error for better prediction.\n\n\n\n\nActivity: In-class discussion\n\n\n\nCan we ever predict \\(Y^*\\) from \\(X^*\\) with zero prediction error? Discuss with your neighbors and report back\n\n\n\n\n\n\n\n\nIn practice, we can not use the test data \\(X^*\\) and \\(Y^*\\) to estimate the prediction error during the training phase because\n\nin many cases, there is no test data,\neven if there is, we are not allowed to use it during the training phase.\n\nTherefore, we can not directly calculate the prediction error, instead we need a method to estimate this quantity using our training data.\n\n\nIn the regression setting, the most commonly-used measure for the quality of fit is the mean squared error (MSE), given by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\nNote that \\(Y= (y_1,y_2,...,y_n)\\), \\(X = (x_1,...,x_n)\\) and \\(x_i = (x_{i1}, x_{i2},...,x_{ip})^T\\).\nCan we use this MSE as an estimate for prediction error?\n\nThis approach usually gives an optimistic estimate of the prediction error. This is not suprising because when training our model to calculate \\(\\hat{f}\\), we are minize this error during the process.\n\n\n\n\n\n\n\nIn linear regression, we have:\n\\[f(X) = \\beta_0 + \\beta_1 X_1 +  \\beta_2 X_2 + ··· + \\beta_p X_p,\\]\nand our goal is to find \\(\\beta_0, \\dots , \\beta_p\\) . The most common approach to fit this linear model is using the the least squares method, which basically minimizes the MSE of the training test.\nTherefore, after computing \\(\\hat{f}\\) through the training phase. So, given new \\(X^*\\),\n\\[\\hat{Y}^* =  \\hat{\\beta_0} + \\hat{\\beta_1} X^*_1 +  \\hat{\\beta_2} X^*_2 + ··· + \\hat{\\beta_p} X^*_p,\\]\nand we can estimate the prediction error by the training error:\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nNote that \\(Y= (y_1,y_2,...,y_n)\\) and \\(\\hat{Y}= (\\hat{y}_1,...,\\hat{y}_n)\\).\n\n\nActivity: Training a Linear Regression Model\n\n\n\nUse the following code to generate a training and test dataset:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(2)\n\n# Define parameters\nn = 50\n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x + 2 * np.random.randn(n)\n\n# Generate test data\nx_s = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny_s = 2 * x_s + 2 * np.random.randn(n)\nNext, we want to let’s fit a Linear Regression model to this data, find the erros, and plot the result. You can use the following code to do so:\n\n# Train a simple linear model\nlm_1 = LinearRegression()\nlm_1.fit(x, y)\n\n# Predictions\nyhat_1 = lm_1.predict(x)\ny_s_hat_1 = lm_1.predict(x_s)\n\n# Compute errors\ntrain_err_1 = np.mean((y - yhat_1) ** 2)\ntest_err_1 = np.mean((y_s - y_s_hat_1) ** 2)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x_s)), max(np.max(x), np.max(x_s)))\nylim = (min(np.min(y), np.min(y_s)), max(np.max(y), np.max(y_s)))\n\n# Plot training data\naxes[0].scatter(x, y, label=\"Training Data\")\naxes[0].plot(x, yhat_1, color='red', linewidth=2, label=\"Fitted Line\")\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\naxes[0].text(0, -6, f\"Training error: {train_err_1:.3f}\", fontsize=12)\n\n# Plot test data\naxes[1].scatter(x_s, y_s, label=\"Test Data\")\naxes[1].plot(x_s, y_s_hat_1, color='green', linewidth=2, label=\"Fitted Line\")\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\naxes[1].text(0, -6, f\"Test error: {test_err_1:.3f}\", fontsize=12)\n\n# Show plots\nplt.show()\nTry that for 10 different random seeds and report back.\n\n\n\nExample of an output:\n\n\n\n\n\n\n\n\n\n\n\n\nHow about if we increase the complexity of our model?\n\n\n\n\n\n\n\n\n\nInterestingly, it can be shown that the prediction error follows the equation:\n\\[\n\\mathbb{E} \\big( Y^* − \\hat{f}(X^*) \\big)^2 = \\text{Var}(\\hat{f}(X^*)) + \\big[ \\text{Bias}(\\hat{f}(X^*)) \\big]^2 + \\text{Var}(\\epsilon)\n\\]\nIn this equation, \\(Var(\\epsilon)\\) represents the irreducible error, which is independent of the data. This means that no matter how good our model is, the prediction error can never be lower than this variance.\nHowever, the prediction error also consists of a bias term and a variance term, both of which depend on the data. The variance measures how much \\(\\hat{f}(X^*)\\) would change if we used a different training dataset. If variance is high, small changes in the training data result in large changes in \\(\\hat{f}\\), leading to overfitting.\nThe bias term represents the error introduced by our choice of model. If we use a very simple model, the bias might be high because it is unlikely that the true data-generating process follows such a simplistic structure. This results in underfitting.\nAs a general rule, more flexible models tend to have higher variance but lower bias, while simpler models have higher bias but lower variance. This fundamental trade-off between bias and variance is known as the bias-variance trade-off.\nIn practice, we often cannot explicitly compute the prediction error, bias, or variance, but we should always keep the bias-variance trade-off in mind when choosing and evaluating models.",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#learning-outcomes",
    "href": "lect1.html#learning-outcomes",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lectures students should be able to:\n\nunderstand the difference between inference and prediction in regression (reminder)\nunderstand the training/test data split\nunderstand the difference between training (in-sample) and test (out-of-sample) prediction errors and define them\nunderstand the imporance of out-of-sample prediction error\nuse Python to estimate out-of-sample prediction error via (in-sample) training error in linear regression given the training data\nunderstand the draw back of estimating the out-of-sample prediction error via training error\nunderstand the Bias-Variance Trade-Off",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#regression",
    "href": "lect1.html#regression",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "We have \\(n\\) data points with \\(p\\) different predictors: \\(X = (X_1, X_2,...,X_p)\\) and a response \\(Y\\), which are independent samples from some distribution.\nWe assume that there is some relationship between which can be written as \\[Y = f(X) + \\epsilon,\\] where \\(f\\) is some fixed but unknown function of \\(X_1,...,X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero.\nOur goal is to estimate \\(f\\) via another function called \\(\\hat{f}\\).\nWhen \\(Y\\) is qualitative, we have a regression problem (and when it is qualitative, we have a classification problem).",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#inference-vs-prediction",
    "href": "lect1.html#inference-vs-prediction",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "Inference: Focuses on identifying relationships and associations between predictors and the response.\nPrediction: Focuses on estimating the response for newly unseen predictors.\n\nOur focus here is on prediction. But how do we know how good we are at this task?",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#trainingtest-split",
    "href": "lect1.html#trainingtest-split",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "When it comes to prediction, we want to work with two different datasets: training and test (aka out-of-sample). The reason for this is that we would then consider a training phase, during which the training data is used to fit (or train) the model, and a testing phase, where the model’s performance is evaluated using the test dataset, which it has never seen during training. This evaluation helps us determine how well the model predicts responses for new, unseen data. From now on, we are going to use \\(X, Y\\) to denote the training data, and \\(X^*, Y^*\\) to denote the test data. We therefore can define the prediciton task more formally by defining it as estimating the regression function \\(\\hat{f}\\) as\n\\[\\hat{Y^*} = \\hat{f}(X^*)\\]\nfor an unseen \\(X^*\\), where \\(X^*, Y^*\\) are sampled from the same distribution as \\(X, Y\\)",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#out-of-sample-prediction-error-in-regression",
    "href": "lect1.html#out-of-sample-prediction-error-in-regression",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "For new test data \\(X^*, Y^*\\) sampled from the same distribution as the trainign data, we can defined the following error term called out-of-sample prediction error or prediction error in regression: \\[\\mathbb{E}(Y^*−\\hat{Y}^*)^2 = \\mathbb{E}(Y^*−\\hat{f}(X^*))^2,\\] where the expectation is taken over the training data \\(X, Y\\), and test data \\(X^*, Y^*\\).\n\n\n\nTo evalute the quality of predicition in our model,\nTo be able to minimize the error for better prediction.\n\n\n\n\nActivity: In-class discussion\n\n\n\nCan we ever predict \\(Y^*\\) from \\(X^*\\) with zero prediction error? Discuss with your neighbors and report back",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#estimating-the-prediction-error",
    "href": "lect1.html#estimating-the-prediction-error",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "In practice, we can not use the test data \\(X^*\\) and \\(Y^*\\) to estimate the prediction error during the training phase because\n\nin many cases, there is no test data,\neven if there is, we are not allowed to use it during the training phase.\n\nTherefore, we can not directly calculate the prediction error, instead we need a method to estimate this quantity using our training data.\n\n\nIn the regression setting, the most commonly-used measure for the quality of fit is the mean squared error (MSE), given by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\nNote that \\(Y= (y_1,y_2,...,y_n)\\), \\(X = (x_1,...,x_n)\\) and \\(x_i = (x_{i1}, x_{i2},...,x_{ip})^T\\).\nCan we use this MSE as an estimate for prediction error?\n\nThis approach usually gives an optimistic estimate of the prediction error. This is not suprising because when training our model to calculate \\(\\hat{f}\\), we are minize this error during the process.",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#example-linear-regression",
    "href": "lect1.html#example-linear-regression",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "In linear regression, we have:\n\\[f(X) = \\beta_0 + \\beta_1 X_1 +  \\beta_2 X_2 + ··· + \\beta_p X_p,\\]\nand our goal is to find \\(\\beta_0, \\dots , \\beta_p\\) . The most common approach to fit this linear model is using the the least squares method, which basically minimizes the MSE of the training test.\nTherefore, after computing \\(\\hat{f}\\) through the training phase. So, given new \\(X^*\\),\n\\[\\hat{Y}^* =  \\hat{\\beta_0} + \\hat{\\beta_1} X^*_1 +  \\hat{\\beta_2} X^*_2 + ··· + \\hat{\\beta_p} X^*_p,\\]\nand we can estimate the prediction error by the training error:\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nNote that \\(Y= (y_1,y_2,...,y_n)\\) and \\(\\hat{Y}= (\\hat{y}_1,...,\\hat{y}_n)\\).\n\n\nActivity: Training a Linear Regression Model\n\n\n\nUse the following code to generate a training and test dataset:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(2)\n\n# Define parameters\nn = 50\n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x + 2 * np.random.randn(n)\n\n# Generate test data\nx_s = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny_s = 2 * x_s + 2 * np.random.randn(n)\nNext, we want to let’s fit a Linear Regression model to this data, find the erros, and plot the result. You can use the following code to do so:\n\n# Train a simple linear model\nlm_1 = LinearRegression()\nlm_1.fit(x, y)\n\n# Predictions\nyhat_1 = lm_1.predict(x)\ny_s_hat_1 = lm_1.predict(x_s)\n\n# Compute errors\ntrain_err_1 = np.mean((y - yhat_1) ** 2)\ntest_err_1 = np.mean((y_s - y_s_hat_1) ** 2)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x_s)), max(np.max(x), np.max(x_s)))\nylim = (min(np.min(y), np.min(y_s)), max(np.max(y), np.max(y_s)))\n\n# Plot training data\naxes[0].scatter(x, y, label=\"Training Data\")\naxes[0].plot(x, yhat_1, color='red', linewidth=2, label=\"Fitted Line\")\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\naxes[0].text(0, -6, f\"Training error: {train_err_1:.3f}\", fontsize=12)\n\n# Plot test data\naxes[1].scatter(x_s, y_s, label=\"Test Data\")\naxes[1].plot(x_s, y_s_hat_1, color='green', linewidth=2, label=\"Fitted Line\")\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\naxes[1].text(0, -6, f\"Test error: {test_err_1:.3f}\", fontsize=12)\n\n# Show plots\nplt.show()\nTry that for 10 different random seeds and report back.\n\n\n\nExample of an output:\n\n\n\n\n\n\n\n\n\n\n\n\nHow about if we increase the complexity of our model?\n\n\n\n\n\n\n\n\n\nInterestingly, it can be shown that the prediction error follows the equation:\n\\[\n\\mathbb{E} \\big( Y^* − \\hat{f}(X^*) \\big)^2 = \\text{Var}(\\hat{f}(X^*)) + \\big[ \\text{Bias}(\\hat{f}(X^*)) \\big]^2 + \\text{Var}(\\epsilon)\n\\]\nIn this equation, \\(Var(\\epsilon)\\) represents the irreducible error, which is independent of the data. This means that no matter how good our model is, the prediction error can never be lower than this variance.\nHowever, the prediction error also consists of a bias term and a variance term, both of which depend on the data. The variance measures how much \\(\\hat{f}(X^*)\\) would change if we used a different training dataset. If variance is high, small changes in the training data result in large changes in \\(\\hat{f}\\), leading to overfitting.\nThe bias term represents the error introduced by our choice of model. If we use a very simple model, the bias might be high because it is unlikely that the true data-generating process follows such a simplistic structure. This results in underfitting.\nAs a general rule, more flexible models tend to have higher variance but lower bias, while simpler models have higher bias but lower variance. This fundamental trade-off between bias and variance is known as the bias-variance trade-off.\nIn practice, we often cannot explicitly compute the prediction error, bias, or variance, but we should always keep the bias-variance trade-off in mind when choosing and evaluating models.",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Lesson plan",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Define parameters\nn = 30\n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n))\ny = 2 * x + 2 * np.random.randn(n)\n\n# Generate test data\nx0 = np.sort(np.random.uniform(-3, 3, n))\ny0 = 2 * x0 + 2 * np.random.randn(n)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x0)), max(np.max(x), np.max(x0)))\nylim = (min(np.min(y), np.min(y0)), max(np.max(y), np.max(y0)))\n\n# Plot training data\naxes[0].scatter(x, y)\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\n\n# Plot test data\naxes[1].scatter(x0, y0)\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\n\n# Show plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(20)\n\n# Generate training data\nn = 50\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Generate test data\nx0 = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny0 = 2 * x0.ravel() + 2 * np.random.randn(n)\n\n# Train a simple linear model\nlm_1 = LinearRegression()\nlm_1.fit(x, y)\n\n# Predictions\nyhat_1 = lm_1.predict(x)\ny0hat_1 = lm_1.predict(x0)\n\n# Compute errors\ntrain_err_1 = np.mean((y - yhat_1) ** 2)\ntest_err_1 = np.mean((y0 - y0hat_1) ** 2)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x0)), max(np.max(x), np.max(x0)))\nylim = (min(np.min(y), np.min(y0)), max(np.max(y), np.max(y0)))\n\n# Plot training data\naxes[0].scatter(x, y, label=\"Training Data\")\naxes[0].plot(x, yhat_1, color='red', linewidth=2, label=\"Fitted Line\")\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\naxes[0].text(0, -6, f\"Training error: {train_err_1:.3f}\", fontsize=12)\n\n# Plot test data\naxes[1].scatter(x0, y0, label=\"Test Data\")\naxes[1].plot(x0, y0hat_1, color='green', linewidth=2, label=\"Fitted Line\")\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\naxes[1].text(0, -6, f\"Test error: {test_err_1:.3f}\", fontsize=12)\n\n# Show plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\n# Set seed for reproducibility\nnp.random.seed(2)\n\n# Generate data\nn = 30\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Prepare cross-validation\nk = 5  # Number of CV folds\nkf = KFold(n_splits=k, shuffle=True, random_state=1)\n\n# Generate smooth x values for plotting fitted models\nxx = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n\n# Initialize prediction matrix (same as `pred.mat` in R)\npred_mat = np.zeros((n, 2))\n\n# Cross-validation loop\nfor i, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n    # Training and test sets\n    x_train, x_test = x[train_idx], x[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n    # Train models\n    lm_1 = LinearRegression()\n    lm_1.fit(x_train, y_train)\n    \n    poly = PolynomialFeatures(degree=10)\n    X_train_poly = poly.fit_transform(x_train)\n    X_test_poly = poly.transform(x_test)\n    XX_poly = poly.transform(xx)\n    \n    lm_10 = LinearRegression()\n    lm_10.fit(X_train_poly, y_train)\n\n    # Predictions\n    y_pred_1 = lm_1.predict(x_test)\n    y_pred_10 = lm_10.predict(X_test_poly)\n\n    # Store test errors\n    train_err = np.mean((y_test - y_pred_1) ** 2)\n    test_err = np.mean((y_test - y_pred_10) ** 2)\n\n    # Plot fitted models\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    cols = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')  # Red for test fold, gray for others\n\n    # Plot 1st order model\n    axes[0].scatter(x, y, c=cols, label=\"Data\")\n    axes[0].plot(xx, lm_1.predict(xx), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"1st Order Fit\")\n    axes[0].set_title(f\"Fold {i} - Linear Model\")\n    axes[0].legend()\n    axes[0].text(0, -6, f\"Fold {i} error: {train_err:.3f}\", fontsize=12)\n\n    # Plot 10th order model\n    axes[1].scatter(x, y, c=cols, label=\"Data\")\n    axes[1].plot(xx, lm_10.predict(XX_poly), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"10th Order Fit\")\n    axes[1].set_title(f\"Fold {i} - 10th Order Model\")\n    axes[1].legend()\n    axes[1].text(0, -6, f\"Fold {i} error: {test_err:.3f}\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100  \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize LOOCV\nloo = LeaveOneOut()\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform LOOCV\nfor train_index, test_index in loo.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the left-out test point\n    y_pred = model.predict(x_test)\n\n    # Compute squared error\n    error = (y_test - y_pred) ** 2\n    errors.append(error)\n    # print(error)\n\n# Compute mean squared error\nloocv_mse = np.mean(errors)\n\n# Print results\nprint(f\"LOOCV Mean Squared Error: {loocv_mse:.4f}\")\n\nLOOCV Mean Squared Error: 3.3991\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize 10-Fold Cross-Validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform 10-Fold Cross-Validation\nfor train_index, test_index in kf.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute mean squared error for this fold\n    error = mean_squared_error(y_test, y_pred)\n    errors.append(error)\n    print(error)\n\n# Compute mean squared error across all folds\nkfold_mse = np.mean(errors)\n\n# Print results\nprint(f\"10-Fold Cross-Validation Mean Squared Error: {kfold_mse:.4f}\")\n\n3.2974245186062086\n2.1281202144087557\n1.822825376783144\n3.355969427701041\n5.3193448694378125\n3.6176524637481853\n3.168061930645366\n2.7906308245233387\n4.350951633463683\n3.681229670826452\n10-Fold Cross-Validation Mean Squared Error: 3.3532\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic dataset\nn = 30  # Number of data points\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() ** 2 - 3 * x.squeeze() + 2 + np.random.randn(n) * 2  # Quadratic relationship with noise\n\n# Define the range of model complexities (polynomial degrees)\ndegrees = np.arange(1, 11)\n\n# Store training and validation errors\ntrain_errors = []\ncv_errors = []\n\n# Loop over polynomial degrees\nfor d in degrees:\n    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n\n    # Compute training error (MSE)\n    model.fit(x, y)\n    y_train_pred = model.predict(x)\n    train_error = np.mean((y - y_train_pred) ** 2)\n    train_errors.append(train_error)\n\n    # Compute cross-validation error (MSE)\n    cv_error = -np.mean(cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=5))\n    cv_errors.append(cv_error)\n\n# Plot the U-shaped Bias-Variance Tradeoff Curve\nplt.figure(figsize=(8, 5))\nplt.plot(degrees, train_errors, label=\"Training Error\", marker=\"o\", linestyle=\"--\")\nplt.plot(degrees, cv_errors, label=\"Cross-Validation Error\", marker=\"s\", linestyle=\"-\")\nplt.xlabel(\"Model Complexity (Polynomial Degree)\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Bias-Variance Tradeoff in Linear Regression\")\nplt.legend()\nplt.ylim(0, max(cv_errors) * 1.2)  # Adjust y-axis limit\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic dataset\nn = 30  # Number of data points\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() ** 2 - 3 * x.squeeze() + 2 + np.random.randn(n) * 2  # Quadratic relationship with noise\n\n# Define the range of model complexities (polynomial degrees)\ndegrees = np.arange(1, 15)\n\n# Store training and test errors\ntrain_errors = []\ntest_errors = []\n\n# Generate test set (unseen data)\nx_test = np.linspace(-3, 3, 100).reshape(-1, 1)\ny_test = 2 * x_test.squeeze() ** 2 - 3 * x_test.squeeze() + 2  # True function without noise\n\n# Loop over polynomial degrees\nfor d in degrees:\n    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n\n    # Train the model\n    model.fit(x, y)\n\n    # Compute training error\n    y_train_pred = model.predict(x)\n    train_error = mean_squared_error(y, y_train_pred)\n    train_errors.append(train_error)\n\n    # Compute test error\n    y_test_pred = model.predict(x_test)\n    test_error = mean_squared_error(y_test, y_test_pred)\n    test_errors.append(test_error)\n\n# Plot the U-shaped Bias-Variance Tradeoff Curve\nplt.figure(figsize=(8, 5))\nplt.plot(degrees, train_errors, label=\"Training Error\", marker=\"o\", linestyle=\"--\")\nplt.plot(degrees, test_errors, label=\"Test Error\", marker=\"s\", linestyle=\"-\")\nplt.xlabel(\"Model Complexity (Polynomial Degree)\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Bias-Variance Tradeoff in Linear Regression\")\nplt.legend()\nplt.ylim(0, max(test_errors) * 1.2)  # Adjust y-axis limit\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "lect2.html",
    "href": "lect2.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lecture students should be able to:\n\nExplain why we should split the training data to get a validation set\nExplain the idea behind cross validation in model training on a high level.\nUse Python to estimate out-of-sample prediction error via Leave-One-Out cross-validation (LOOCV) given training data\nUse Python to etimate out-of-sample prediction error via \\(K\\)-Fold cross-validation given training data\nExplain the pros and cons of LOOCV and \\(K\\)-Fold cross-validation when used in regression\nExplain Bias-Variance Trade-Off for \\(K\\)-Fold cross-validation when used in regression\n\n\n\n\nIf we use all of our taining data to fit our regression models, estimating prediction error with in-sample (training) error in essentially the best thing we can do. To avoid this, a common approach to evaluate model perfomance is to split the available data into a (smaller) training set and a validation set. We can then fit our model using only the smaller training set and evaluate its performance by predicting the responses for the validation set. This allows us to simulate the model’s prediction error on unseen data while still having access to the true responses outside the test set.\n\n\n\nOne way to split the training data to get a validation set is to hold out one of the data points. We can then train the model using the remaining \\((n−1)\\) data points and estimate the prediction error by\n\\[(y_n - \\hat{f}^{-n}(x_n))^2,\\]\nwhere \\(\\hat{f}^{-n}\\) denote the regression function estimate using all by the data point \\((y_n, x_n)\\).\nHowever, this estimate highly depends on the chosen point and can be very high variance.\n\n\n\nActivity: In-class discussion\n\n\n\nIs \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) a reasonable estimate for the out-of-sample prediction error? Discuss with your neighbors and report back.\n\n\n\n\n\n\nOne way to modify the above approach to reduce the variability is to repeat the above process for every single point and look at the average of error estiamtes, i.e., estiamte the prediction error by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}^{-i}(x_i))^2.\\]\nThis approach is called ed leave-one-out cross-validation (LOOCV). Notice that we are basically estimating \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) empirically here. Also, since we are repeating training process \\(n\\) times the variance of our prediction is highly reduced, but since the size of our dataset during the training phase is roughly the same size is the original training set, LOOCV increases the computational load by a factor of \\(n\\).\n\n\n\nOne further modifcation to Leave-one-out cross-validation is instead of holding out 1 data point at a time and repeat this process \\(n\\) times, slit the training data into \\(k\\) folds \\(F_1,...,F_k\\), fit our regression model using data from \\((k-1)\\) folds and calculate the error for the \\(k\\)th fold using:\n\\[\\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2,\\]\nwhere \\(n_k = |F_k|\\), and \\(\\hat{f}^{-(k)}\\) denotes the regression fuction trained using all folds but the \\(k\\)th one. We can then repeate this process \\(k\\) times to estimate our prediction error by taking over over the error calculated for each fold\n\\[\\frac{1}{k} \\sum_{i=1}^k \\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2.\\]\nThis method is called \\(K\\)-fold cross-validation\n\n\n\nWe can run LOOCV in Python by using the function LeaveOneOut from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize LOOCV\nloo = LeaveOneOut()\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform LOOCV\nfor train_index, test_index in loo.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the left-out test point\n    y_pred = model.predict(x_test)\n\n    # Compute squared error\n    error = (y_test - y_pred) ** 2\n    errors.append(error)\n\n# Compute mean squared error\nloocv_mse = np.mean(errors)\n\n# Print results\nprint(f\"LOOCV Mean Squared Error: {loocv_mse:.4f}\")\n\nLOOCV Mean Squared Error: 3.3991\n\n\n\n\n\nWe can run LOOCV in Python by using the function KFold from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize 10-Fold Cross-Validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform 10-Fold Cross-Validation\nfor train_index, test_index in kf.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute mean squared error for this fold\n    error = mean_squared_error(y_test, y_pred)\n    errors.append(error)\n    # print(error)\n\n# Compute mean squared error across all folds\nkfold_mse = np.mean(errors)\n\n# Print results\nprint(f\"10-Fold Cross-Validation Mean Squared Error: {kfold_mse:.4f}\")\n\n10-Fold Cross-Validation Mean Squared Error: 3.3532",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#learning-outcomes",
    "href": "lect2.html#learning-outcomes",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lecture students should be able to:\n\nExplain why we should split the training data to get a validation set\nExplain the idea behind cross validation in model training on a high level.\nUse Python to estimate out-of-sample prediction error via Leave-One-Out cross-validation (LOOCV) given training data\nUse Python to etimate out-of-sample prediction error via \\(K\\)-Fold cross-validation given training data\nExplain the pros and cons of LOOCV and \\(K\\)-Fold cross-validation when used in regression\nExplain Bias-Variance Trade-Off for \\(K\\)-Fold cross-validation when used in regression",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#splitting-the-training-data",
    "href": "lect2.html#splitting-the-training-data",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "If we use all of our taining data to fit our regression models, estimating prediction error with in-sample (training) error in essentially the best thing we can do. To avoid this, a common approach to evaluate model perfomance is to split the available data into a (smaller) training set and a validation set. We can then fit our model using only the smaller training set and evaluate its performance by predicting the responses for the validation set. This allows us to simulate the model’s prediction error on unseen data while still having access to the true responses outside the test set.",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#a-single-held-out-point",
    "href": "lect2.html#a-single-held-out-point",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "One way to split the training data to get a validation set is to hold out one of the data points. We can then train the model using the remaining \\((n−1)\\) data points and estimate the prediction error by\n\\[(y_n - \\hat{f}^{-n}(x_n))^2,\\]\nwhere \\(\\hat{f}^{-n}\\) denote the regression function estimate using all by the data point \\((y_n, x_n)\\).\nHowever, this estimate highly depends on the chosen point and can be very high variance.\n\n\n\nActivity: In-class discussion\n\n\n\nIs \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) a reasonable estimate for the out-of-sample prediction error? Discuss with your neighbors and report back.",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#leave-one-out-cross-validation",
    "href": "lect2.html#leave-one-out-cross-validation",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "One way to modify the above approach to reduce the variability is to repeat the above process for every single point and look at the average of error estiamtes, i.e., estiamte the prediction error by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}^{-i}(x_i))^2.\\]\nThis approach is called ed leave-one-out cross-validation (LOOCV). Notice that we are basically estimating \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) empirically here. Also, since we are repeating training process \\(n\\) times the variance of our prediction is highly reduced, but since the size of our dataset during the training phase is roughly the same size is the original training set, LOOCV increases the computational load by a factor of \\(n\\).",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#k-fold-cross-validation",
    "href": "lect2.html#k-fold-cross-validation",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "One further modifcation to Leave-one-out cross-validation is instead of holding out 1 data point at a time and repeat this process \\(n\\) times, slit the training data into \\(k\\) folds \\(F_1,...,F_k\\), fit our regression model using data from \\((k-1)\\) folds and calculate the error for the \\(k\\)th fold using:\n\\[\\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2,\\]\nwhere \\(n_k = |F_k|\\), and \\(\\hat{f}^{-(k)}\\) denotes the regression fuction trained using all folds but the \\(k\\)th one. We can then repeate this process \\(k\\) times to estimate our prediction error by taking over over the error calculated for each fold\n\\[\\frac{1}{k} \\sum_{i=1}^k \\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2.\\]\nThis method is called \\(K\\)-fold cross-validation",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#how-to-perform-loocv-in-python",
    "href": "lect2.html#how-to-perform-loocv-in-python",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "We can run LOOCV in Python by using the function LeaveOneOut from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize LOOCV\nloo = LeaveOneOut()\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform LOOCV\nfor train_index, test_index in loo.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the left-out test point\n    y_pred = model.predict(x_test)\n\n    # Compute squared error\n    error = (y_test - y_pred) ** 2\n    errors.append(error)\n\n# Compute mean squared error\nloocv_mse = np.mean(errors)\n\n# Print results\nprint(f\"LOOCV Mean Squared Error: {loocv_mse:.4f}\")\n\nLOOCV Mean Squared Error: 3.3991",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#how-to-perform-k-fold-cross-validation-in-python",
    "href": "lect2.html#how-to-perform-k-fold-cross-validation-in-python",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "We can run LOOCV in Python by using the function KFold from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize 10-Fold Cross-Validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform 10-Fold Cross-Validation\nfor train_index, test_index in kf.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute mean squared error for this fold\n    error = mean_squared_error(y_test, y_pred)\n    errors.append(error)\n    # print(error)\n\n# Compute mean squared error across all folds\nkfold_mse = np.mean(errors)\n\n# Print results\nprint(f\"10-Fold Cross-Validation Mean Squared Error: {kfold_mse:.4f}\")\n\n10-Fold Cross-Validation Mean Squared Error: 3.3532",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]