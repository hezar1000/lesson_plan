[
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "An Introduction to Statistical Learning with Applications in Python, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2023.\nLecture Notes on Statistical Prediction, Ryan Tibshirani, 2021.\nLecture notes on Error and Validation, Ryan Tibshirani, 2014.\nI also used ChatGPT for brainstorming, proof-reading, and genereating code.",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "ref.html#references",
    "href": "ref.html#references",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "An Introduction to Statistical Learning with Applications in Python, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2023.\nLecture Notes on Statistical Prediction, Ryan Tibshirani, 2021.\nLecture notes on Error and Validation, Ryan Tibshirani, 2014.\nI also used ChatGPT for brainstorming, proof-reading, and genereating code.",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "When designing this lesson plan, I’ve assumed that students\n\nhave taken introductory courses in probability, calculus, and linear algebra.\nhave taken courses in statistical learning but have mostly focused on inference rather than prediction.\n\nare comfortable writing code in Python and have used libraries such as numpy, pandas, matplotlib, and sklearn in previous courses (e.g., to fit a linear regression model) but have not used them for more advanced tasks such as cross-validation.\nare comfortable with running Python code in a jupyter lab environment on their own.\nare familiar with the matrix notation used in statistical learning.",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "index.html#assumptions-about-students",
    "href": "index.html#assumptions-about-students",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "When designing this lesson plan, I’ve assumed that students\n\nhave taken introductory courses in probability, calculus, and linear algebra.\nhave taken courses in statistical learning but have mostly focused on inference rather than prediction.\n\nare comfortable writing code in Python and have used libraries such as numpy, pandas, matplotlib, and sklearn in previous courses (e.g., to fit a linear regression model) but have not used them for more advanced tasks such as cross-validation.\nare comfortable with running Python code in a jupyter lab environment on their own.\nare familiar with the matrix notation used in statistical learning.",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of this lesson, students should be able to:\n\nIdentify the difference between inference and prediction in regression.\nUnderstand the training/test/validation data split.\nDefine out-of-sample prediction error and explain its importance.\nUse Python to estimate out-of-sample prediction error via cross-validation given training data.\nUnderstand the Bias-Variance Trade-Off and its relationship to prediction error.",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "Outline",
    "text": "Outline\n\nLecture 1: Basics\n\nRegression definition\nInference vs Prediction\nTraining/test split\nOut-of-sample prediction error in regression\n\nDefinition\nWhy do we care about estimating out-of-sample error?\nIn-class discussion: Can we ever make a prediction with zero prediction error?\n\nGoals:\n\nGive students a chance to digest and reflect on the topics covered so far\nRemind them of reducible and irreducible error\n\n\n\nEstimating the prediction error\n\nMotivation\nFirst approach: Estimating prediction error with in-sample (training) error\n\nExample: Linear Regression\n\nIn-class live-coding activity: Calculate and compare the training and test errors using different seeds.\n\nGoals:\n\nGive students a chance to play with training and test errors on a simple regression model\nGive them a chance to observe how different they can be\n\n\n\n\n\nThe Bias-Variance Trade-Off\n\nMotivating example: 10th order polynomial regression\nOverfitting vs underfitting\nDefinition\n\n\n\n\nLecture 2: Cross-validation\n\nEstimating the prediction error (continued)\n\nSplitting the training data to get a validation set\nSecond approach: A single held-out point\n\nIn-class discussion: Discussion of the estimate for the out-of-sample prediction error via a single held-out point\n\nGoals:\n\nGive students a chance to digest the idea of a validation set\nMake them more comfortable with estimating the prediction error with training data\n\n\n\nThird approach: Cross-validation\n\nLOOCV\nK-fold cross-validation\n\nImplementing cross-validation in Python\n\nHow to perform LOOCV in Python\nHow to perform \\(K\\)-fold cross-validation in Python\n\nIn-class live-coding activity: \\(K\\)-fold cross-validation\n\nGoals:\n\nLet students play with \\(K\\)-fold cross-validation and get comfortable with the idea\nGet a sense for Bias-Variance Trade-Off for \\(K\\)-Fold cross-validation via an example\n\n\n\nBias-Variance Trade-Off for K-fold cross-validation",
    "crumbs": [
      "Home",
      "Learning outcomes/Outline"
    ]
  },
  {
    "objectID": "lect1.html",
    "href": "lect1.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lecture, students should be able to:\n\nDifferentiate between inference and prediction in regression at a conceptual level.\n\nUnderstand the reasoning behind splitting data into training and test sets for model evaluation.\n\nDistinguish between training (in-sample) error and test (out-of-sample) error, and explain their significance.\n\nUse Python to estimate out-of-sample prediction error using training error in linear regression.\n\nDefine and identify the concepts of overfitting and underfitting in predictive modeling.\n\nExplain the Bias-Variance Trade-Off and its implications for model performance.\n\n\n\n\n\nWe have \\(n\\) data points with \\(p\\) different predictors: \\(X = (X_1, X_2, ..., X_p)\\) and a response \\(Y\\), which are independent samples from some distribution.\nWe assume that there is some relationship between them, which can be written as \\[Y = f(X) + \\epsilon,\\] where \\(f\\) is some fixed but unknown function of \\(X_1, ..., X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero.\nOur goal is to estimate \\(f\\) via another function called \\(\\hat{f}\\).\nWhen \\(Y\\) is quantitative, we have a regression problem (and when it is qualitative, we have a classification problem).\n\n\n\n\n\n\n\nInference: Focuses on identifying relationships and associations between predictors and the response.\nPrediction: Focuses on estimating the response for newly unseen predictors.\n\nOur focus here is on prediction. But how do we know how good we are at this task?\n\n\n\n\nWhen it comes to prediction, we want to work with two different datasets: training and test (aka out-of-sample). The reason for this is that we would then consider a training phase, during which the training data is used to fit (or train) the model, and a testing phase, where the model’s performance is evaluated using the test dataset, which it has never seen during training. This evaluation helps us determine how well the model predicts responses for new, unseen data. From now on, we are going to use \\(X, Y\\) to denote the training data, and \\(X^*, Y^*\\) to denote the test data. We therefore can define the prediciton task more formally by defining it as estimating the regression function \\(\\hat{f}\\) as\n\\[\\hat{Y^*} = \\hat{f}(X^*)\\]\nfor an unseen \\(X^*\\), where \\(X^*, Y^*\\) are sampled from the same distribution as \\(X, Y\\)\n\n\n\nFor new test data \\(X^*, Y^*\\) sampled from the same distribution as the trainign data, we can defined the following error term called out-of-sample prediction error or prediction error in regression: \\[\\mathbb{E}(Y^*−\\hat{Y}^*)^2 = \\mathbb{E}(Y^*−\\hat{f}(X^*))^2,\\] where the expectation is taken over the training data \\(X, Y\\), and test data \\(X^*, Y^*\\).\n\n\n\nTo evalute the quality of predicition in our model,\nTo be able to minimize the error for better prediction.\n\nExample: Imagine to you’ve trained a model to predict stock market. Think about how much you care about you model being accurate in the unseen future.\n\n\n\nActivity: In-class discussion\n\n\n\nCan we ever predict \\(Y^*\\) from \\(X^*\\) with zero prediction error? Discuss with your neighbors and report back\n\n\n\n\n\n\n\n\nIn practice, we cannot use the test data \\(X^*\\) and \\(Y^*\\) to estimate the prediction error during the training phase because:\n\nIn many cases, there is no test data.\nEven if there is, we are not allowed to use it during the training phase.\n\nTherefore, we cannot directly calculate the prediction error; instead, we need a method to estimate this quantity using our training data.\n\n\nIn the regression setting, the most commonly-used measure for the quality of fit is the mean squared error (MSE), given by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\nNote that \\(Y= (y_1,y_2,...,y_n)\\), \\(X = (x_1,...,x_n)\\), and \\(x_i = (x_{i1}, x_{i2},...,x_{ip})^T\\).\nCan we use this MSE as an estimate for prediction error?\n\nThis approach usually gives an optimistic estimate of the prediction error. This is not surprising because when training our model to calculate \\(\\hat{f}\\), we are minimizing this error during the process.\n\n\n\n\n\n\n\nIn linear regression, we have:\n\\[f(X) = \\beta_0 + \\beta_1 X_1 +  \\beta_2 X_2 + \\dots + \\beta_p X_p,\\]\nand our goal is to find \\(\\beta_0, \\dots , \\beta_p\\). The most common approach to fit this linear model is using the least squares method, which essentially minimizes the MSE of the training set.\nTherefore, after computing \\(\\hat{f}\\) through the training phase, given new \\(X^*\\), we have:\n\\[\\hat{Y}^* = \\hat{\\beta_0} + \\hat{\\beta_1} X^*_1 + \\hat{\\beta_2} X^*_2 + \\dots + \\hat{\\beta_p} X^*_p,\\]\nand we can estimate the prediction error using the training error:\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nNote that \\(Y = (y_1, y_2, \\dots, y_n)\\) and \\(\\hat{Y} = (\\hat{y}_1, \\dots, \\hat{y}_n)\\).\n\n\nActivity: Training a Linear Regression Model\n\n\n\nUse the following code to generate a training and test dataset:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(2)\n\n# Define parameters\nn = 50\n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x + 2 * np.random.randn(n)\n\n# Generate test data\nx_s = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny_s = 2 * x_s + 2 * np.random.randn(n)\nNext, we want to let’s fit a Linear Regression model to this data, find the erros, and plot the result. You can use the following code to do so:\n\n# Train a simple linear model\nlm_1 = LinearRegression()\nlm_1.fit(x, y)\n\n# Predictions\nyhat_1 = lm_1.predict(x)\ny_s_hat_1 = lm_1.predict(x_s)\n\n# Compute errors\ntrain_err_1 = np.mean((y - yhat_1) ** 2)\ntest_err_1 = np.mean((y_s - y_s_hat_1) ** 2)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x_s)), max(np.max(x), np.max(x_s)))\nylim = (min(np.min(y), np.min(y_s)), max(np.max(y), np.max(y_s)))\n\n# Plot training data\naxes[0].scatter(x, y, label=\"Training Data\")\naxes[0].plot(x, yhat_1, color='red', linewidth=2, label=\"Fitted Line\")\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\naxes[0].text(0, -6, f\"Training error: {train_err_1:.3f}\", fontsize=12)\n\n# Plot test data\naxes[1].scatter(x_s, y_s, label=\"Test Data\")\naxes[1].plot(x_s, y_s_hat_1, color='green', linewidth=2, label=\"Fitted Line\")\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\naxes[1].text(0, -6, f\"Test error: {test_err_1:.3f}\", fontsize=12)\n\n# Show plots\nplt.show()\nTry that for 10 different random seeds and report back.\n\n\n\nExample of an output:\n\n\n\n\n\n\n\n\n\n\n\n\nHow about if we increase the complexity of our model?\n\n\n\n\n\n\n\n\n\nInterestingly, it can be shown that the prediction error follows the equation:\n\\[\n\\mathbb{E} \\big( Y^* − \\hat{f}(X^*) \\big)^2 = \\text{Var}(\\hat{f}(X^*)) + \\big[ \\text{Bias}(\\hat{f}(X^*)) \\big]^2 + \\text{Var}(\\epsilon)\n\\]\nIn this equation, \\(Var(\\epsilon)\\) represents the irreducible error, which is independent of the data. This means that no matter how good our model is, the prediction error can never be lower than this variance.\nHowever, the prediction error also consists of a bias term and a variance term, both of which depend on the data. The variance measures how much \\(\\hat{f}(X^*)\\) would change if we used a different training dataset. If variance is high, small changes in the training data result in large changes in \\(\\hat{f}\\), leading to overfitting.\n\nExtreme example of overfitting: A curve that goes through every training datapoint\n\nThe bias term represents the error introduced by our choice of model. If we use a very simple model, the bias might be high because it is unlikely that the true data-generating process follows such a simplistic structure. This results in underfitting.\n\nExtreme example of overfitting: A horizontal line\n\nAs a general rule, more flexible models tend to have higher variance but lower bias, while simpler models have higher bias but lower variance. This fundamental trade-off between bias and variance is known as the bias-variance trade-off.\nIn practice, as mentioned, we often cannot explicitly compute the prediction error, bias, or variance, so, we should always keep the bias-variance trade-off in mind when choosing and evaluating models.\nExample: bias-variance trade-off in a regression modes used to fit data with Quadratic relationship",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#learning-outcomes",
    "href": "lect1.html#learning-outcomes",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lecture, students should be able to:\n\nDifferentiate between inference and prediction in regression at a conceptual level.\n\nUnderstand the reasoning behind splitting data into training and test sets for model evaluation.\n\nDistinguish between training (in-sample) error and test (out-of-sample) error, and explain their significance.\n\nUse Python to estimate out-of-sample prediction error using training error in linear regression.\n\nDefine and identify the concepts of overfitting and underfitting in predictive modeling.\n\nExplain the Bias-Variance Trade-Off and its implications for model performance.",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#regression",
    "href": "lect1.html#regression",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "We have \\(n\\) data points with \\(p\\) different predictors: \\(X = (X_1, X_2, ..., X_p)\\) and a response \\(Y\\), which are independent samples from some distribution.\nWe assume that there is some relationship between them, which can be written as \\[Y = f(X) + \\epsilon,\\] where \\(f\\) is some fixed but unknown function of \\(X_1, ..., X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero.\nOur goal is to estimate \\(f\\) via another function called \\(\\hat{f}\\).\nWhen \\(Y\\) is quantitative, we have a regression problem (and when it is qualitative, we have a classification problem).",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#inference-vs-prediction",
    "href": "lect1.html#inference-vs-prediction",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "Inference: Focuses on identifying relationships and associations between predictors and the response.\nPrediction: Focuses on estimating the response for newly unseen predictors.\n\nOur focus here is on prediction. But how do we know how good we are at this task?",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#trainingtest-split",
    "href": "lect1.html#trainingtest-split",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "When it comes to prediction, we want to work with two different datasets: training and test (aka out-of-sample). The reason for this is that we would then consider a training phase, during which the training data is used to fit (or train) the model, and a testing phase, where the model’s performance is evaluated using the test dataset, which it has never seen during training. This evaluation helps us determine how well the model predicts responses for new, unseen data. From now on, we are going to use \\(X, Y\\) to denote the training data, and \\(X^*, Y^*\\) to denote the test data. We therefore can define the prediciton task more formally by defining it as estimating the regression function \\(\\hat{f}\\) as\n\\[\\hat{Y^*} = \\hat{f}(X^*)\\]\nfor an unseen \\(X^*\\), where \\(X^*, Y^*\\) are sampled from the same distribution as \\(X, Y\\)",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#out-of-sample-prediction-error-in-regression",
    "href": "lect1.html#out-of-sample-prediction-error-in-regression",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "For new test data \\(X^*, Y^*\\) sampled from the same distribution as the trainign data, we can defined the following error term called out-of-sample prediction error or prediction error in regression: \\[\\mathbb{E}(Y^*−\\hat{Y}^*)^2 = \\mathbb{E}(Y^*−\\hat{f}(X^*))^2,\\] where the expectation is taken over the training data \\(X, Y\\), and test data \\(X^*, Y^*\\).\n\n\n\nTo evalute the quality of predicition in our model,\nTo be able to minimize the error for better prediction.\n\nExample: Imagine to you’ve trained a model to predict stock market. Think about how much you care about you model being accurate in the unseen future.\n\n\n\nActivity: In-class discussion\n\n\n\nCan we ever predict \\(Y^*\\) from \\(X^*\\) with zero prediction error? Discuss with your neighbors and report back",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#estimating-the-prediction-error",
    "href": "lect1.html#estimating-the-prediction-error",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "In practice, we cannot use the test data \\(X^*\\) and \\(Y^*\\) to estimate the prediction error during the training phase because:\n\nIn many cases, there is no test data.\nEven if there is, we are not allowed to use it during the training phase.\n\nTherefore, we cannot directly calculate the prediction error; instead, we need a method to estimate this quantity using our training data.\n\n\nIn the regression setting, the most commonly-used measure for the quality of fit is the mean squared error (MSE), given by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\nNote that \\(Y= (y_1,y_2,...,y_n)\\), \\(X = (x_1,...,x_n)\\), and \\(x_i = (x_{i1}, x_{i2},...,x_{ip})^T\\).\nCan we use this MSE as an estimate for prediction error?\n\nThis approach usually gives an optimistic estimate of the prediction error. This is not surprising because when training our model to calculate \\(\\hat{f}\\), we are minimizing this error during the process.",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "lect1.html#example-linear-regression",
    "href": "lect1.html#example-linear-regression",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "In linear regression, we have:\n\\[f(X) = \\beta_0 + \\beta_1 X_1 +  \\beta_2 X_2 + \\dots + \\beta_p X_p,\\]\nand our goal is to find \\(\\beta_0, \\dots , \\beta_p\\). The most common approach to fit this linear model is using the least squares method, which essentially minimizes the MSE of the training set.\nTherefore, after computing \\(\\hat{f}\\) through the training phase, given new \\(X^*\\), we have:\n\\[\\hat{Y}^* = \\hat{\\beta_0} + \\hat{\\beta_1} X^*_1 + \\hat{\\beta_2} X^*_2 + \\dots + \\hat{\\beta_p} X^*_p,\\]\nand we can estimate the prediction error using the training error:\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nNote that \\(Y = (y_1, y_2, \\dots, y_n)\\) and \\(\\hat{Y} = (\\hat{y}_1, \\dots, \\hat{y}_n)\\).\n\n\nActivity: Training a Linear Regression Model\n\n\n\nUse the following code to generate a training and test dataset:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(2)\n\n# Define parameters\nn = 50\n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x + 2 * np.random.randn(n)\n\n# Generate test data\nx_s = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny_s = 2 * x_s + 2 * np.random.randn(n)\nNext, we want to let’s fit a Linear Regression model to this data, find the erros, and plot the result. You can use the following code to do so:\n\n# Train a simple linear model\nlm_1 = LinearRegression()\nlm_1.fit(x, y)\n\n# Predictions\nyhat_1 = lm_1.predict(x)\ny_s_hat_1 = lm_1.predict(x_s)\n\n# Compute errors\ntrain_err_1 = np.mean((y - yhat_1) ** 2)\ntest_err_1 = np.mean((y_s - y_s_hat_1) ** 2)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x_s)), max(np.max(x), np.max(x_s)))\nylim = (min(np.min(y), np.min(y_s)), max(np.max(y), np.max(y_s)))\n\n# Plot training data\naxes[0].scatter(x, y, label=\"Training Data\")\naxes[0].plot(x, yhat_1, color='red', linewidth=2, label=\"Fitted Line\")\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\naxes[0].text(0, -6, f\"Training error: {train_err_1:.3f}\", fontsize=12)\n\n# Plot test data\naxes[1].scatter(x_s, y_s, label=\"Test Data\")\naxes[1].plot(x_s, y_s_hat_1, color='green', linewidth=2, label=\"Fitted Line\")\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\naxes[1].text(0, -6, f\"Test error: {test_err_1:.3f}\", fontsize=12)\n\n# Show plots\nplt.show()\nTry that for 10 different random seeds and report back.\n\n\n\nExample of an output:\n\n\n\n\n\n\n\n\n\n\n\n\nHow about if we increase the complexity of our model?\n\n\n\n\n\n\n\n\n\nInterestingly, it can be shown that the prediction error follows the equation:\n\\[\n\\mathbb{E} \\big( Y^* − \\hat{f}(X^*) \\big)^2 = \\text{Var}(\\hat{f}(X^*)) + \\big[ \\text{Bias}(\\hat{f}(X^*)) \\big]^2 + \\text{Var}(\\epsilon)\n\\]\nIn this equation, \\(Var(\\epsilon)\\) represents the irreducible error, which is independent of the data. This means that no matter how good our model is, the prediction error can never be lower than this variance.\nHowever, the prediction error also consists of a bias term and a variance term, both of which depend on the data. The variance measures how much \\(\\hat{f}(X^*)\\) would change if we used a different training dataset. If variance is high, small changes in the training data result in large changes in \\(\\hat{f}\\), leading to overfitting.\n\nExtreme example of overfitting: A curve that goes through every training datapoint\n\nThe bias term represents the error introduced by our choice of model. If we use a very simple model, the bias might be high because it is unlikely that the true data-generating process follows such a simplistic structure. This results in underfitting.\n\nExtreme example of overfitting: A horizontal line\n\nAs a general rule, more flexible models tend to have higher variance but lower bias, while simpler models have higher bias but lower variance. This fundamental trade-off between bias and variance is known as the bias-variance trade-off.\nIn practice, as mentioned, we often cannot explicitly compute the prediction error, bias, or variance, so, we should always keep the bias-variance trade-off in mind when choosing and evaluating models.\nExample: bias-variance trade-off in a regression modes used to fit data with Quadratic relationship",
    "crumbs": [
      "Home",
      "Lecture 1: Basics"
    ]
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Lesson plan",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Define parameters\nn = 30\n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n))\ny = 2 * x + 2 * np.random.randn(n)\n\n# Generate test data\nx0 = np.sort(np.random.uniform(-3, 3, n))\ny0 = 2 * x0 + 2 * np.random.randn(n)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x0)), max(np.max(x), np.max(x0)))\nylim = (min(np.min(y), np.min(y0)), max(np.max(y), np.max(y0)))\n\n# Plot training data\naxes[0].scatter(x, y)\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\n\n# Plot test data\naxes[1].scatter(x0, y0)\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\n\n# Show plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(20)\n\n# Generate training data\nn = 50\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Generate test data\nx0 = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny0 = 2 * x0.ravel() + 2 * np.random.randn(n)\n\n# Train a simple linear model\nlm_1 = LinearRegression()\nlm_1.fit(x, y)\n\n# Predictions\nyhat_1 = lm_1.predict(x)\ny0hat_1 = lm_1.predict(x0)\n\n# Compute errors\ntrain_err_1 = np.mean((y - yhat_1) ** 2)\ntest_err_1 = np.mean((y0 - y0hat_1) ** 2)\n\n# Set up plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Define axis limits\nxlim = (min(np.min(x), np.min(x0)), max(np.max(x), np.max(x0)))\nylim = (min(np.min(y), np.min(y0)), max(np.max(y), np.max(y0)))\n\n# Plot training data\naxes[0].scatter(x, y, label=\"Training Data\")\naxes[0].plot(x, yhat_1, color='red', linewidth=2, label=\"Fitted Line\")\naxes[0].set_xlim(xlim)\naxes[0].set_ylim(ylim)\naxes[0].set_title(\"Training data\")\naxes[0].text(0, -6, f\"Training error: {train_err_1:.3f}\", fontsize=12)\n\n# Plot test data\naxes[1].scatter(x0, y0, label=\"Test Data\")\naxes[1].plot(x0, y0hat_1, color='green', linewidth=2, label=\"Fitted Line\")\naxes[1].set_xlim(xlim)\naxes[1].set_ylim(ylim)\naxes[1].set_title(\"Test data\")\naxes[1].text(0, -6, f\"Test error: {test_err_1:.3f}\", fontsize=12)\n\n# Show plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Generate data\nn = 30\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Prepare cross-validation\nk = 5  # Number of CV folds\nkf = KFold(n_splits=k, shuffle=True, random_state=1)\n\n# Generate smooth x values for plotting fitted models\nxx = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n\n# Initialize prediction matrix (same as `pred.mat` in R)\npred_mat = np.zeros((n, 2))\n\n# Cross-validation loop\nfor i, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n    # Training and test sets\n    x_train, x_test = x[train_idx], x[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n    # Train models\n    lm_1 = LinearRegression()\n    lm_1.fit(x_train, y_train)\n    \n    poly = PolynomialFeatures(degree=10)\n    X_train_poly = poly.fit_transform(x_train)\n    X_test_poly = poly.transform(x_test)\n    XX_poly = poly.transform(xx)\n    \n    lm_10 = LinearRegression()\n    lm_10.fit(X_train_poly, y_train)\n\n    # Predictions\n    y_pred_1 = lm_1.predict(x_test)\n    y_pred_10 = lm_10.predict(X_test_poly)\n\n    # Store test errors\n    train_err = np.mean((y_test - y_pred_1) ** 2)\n    test_err = np.mean((y_test - y_pred_10) ** 2)\n\n    # Plot fitted models\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    cols = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')  # Red for test fold, gray for others\n\n    # Plot 1st order model\n    axes[0].scatter(x, y, c=cols, label=\"Data\")\n    axes[0].plot(xx, lm_1.predict(xx), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"1st Order Fit\")\n    axes[0].set_title(f\"Fold {i} - Linear Model\")\n    axes[0].legend()\n    axes[0].text(0, -6, f\"Fold {i} error: {train_err:.3f}\", fontsize=12)\n\n    # Plot 10th order model\n    axes[1].scatter(x, y, c=cols, label=\"Data\")\n    axes[1].plot(xx, lm_10.predict(XX_poly), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"10th Order Fit\")\n    axes[1].set_title(f\"Fold {i} - 10th Order Model\")\n    axes[1].legend()\n    axes[1].text(0, -6, f\"Fold {i} error: {test_err:.3f}\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100  \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize LOOCV\nloo = LeaveOneOut()\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform LOOCV\nfor train_index, test_index in loo.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the left-out test point\n    y_pred = model.predict(x_test)\n\n    # Compute squared error\n    error = (y_test - y_pred) ** 2\n    errors.append(error)\n    # print(error)\n\n# Compute mean squared error\nloocv_mse = np.mean(errors)\n\n# Print results\nprint(f\"LOOCV Mean Squared Error: {loocv_mse:.4f}\")\n\nLOOCV Mean Squared Error: 3.3991\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize 10-Fold Cross-Validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform 10-Fold Cross-Validation\nfor train_index, test_index in kf.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute mean squared error for this fold\n    error = mean_squared_error(y_test, y_pred)\n    errors.append(error)\n    print(error)\n\n# Compute mean squared error across all folds\nkfold_mse = np.mean(errors)\n\n# Print results\nprint(f\"10-Fold Cross-Validation Mean Squared Error: {kfold_mse:.4f}\")\n\n3.2974245186062086\n2.1281202144087557\n1.822825376783144\n3.355969427701041\n5.3193448694378125\n3.6176524637481853\n3.168061930645366\n2.7906308245233387\n4.350951633463683\n3.681229670826452\n10-Fold Cross-Validation Mean Squared Error: 3.3532\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic dataset\nn = 30  # Number of data points\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() ** 2 - 3 * x.squeeze() + 2 + np.random.randn(n) * 2  # Quadratic relationship with noise\n\n# Define the range of model complexities (polynomial degrees)\ndegrees = np.arange(1, 11)\n\n# Store training and validation errors\ntrain_errors = []\ncv_errors = []\n\n# Loop over polynomial degrees\nfor d in degrees:\n    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n\n    # Compute training error (MSE)\n    model.fit(x, y)\n    y_train_pred = model.predict(x)\n    train_error = np.mean((y - y_train_pred) ** 2)\n    train_errors.append(train_error)\n\n    # Compute cross-validation error (MSE)\n    cv_error = -np.mean(cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=5))\n    cv_errors.append(cv_error)\n\n# Plot the U-shaped Bias-Variance Tradeoff Curve\nplt.figure(figsize=(8, 5))\nplt.plot(degrees, train_errors, label=\"Training Error\", marker=\"o\", linestyle=\"--\")\nplt.plot(degrees, cv_errors, label=\"Cross-Validation Error\", marker=\"s\", linestyle=\"-\")\nplt.xlabel(\"Model Complexity (Polynomial Degree)\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Bias-Variance Tradeoff in Linear Regression\")\nplt.legend()\nplt.ylim(0, max(cv_errors) * 1.2)  # Adjust y-axis limit\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic dataset\nn = 30  # Number of data points\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() ** 2 - 3 * x.squeeze() + 2 + np.random.randn(n) * 2  # Quadratic relationship with noise\n\n# Define the range of model complexities (polynomial degrees)\ndegrees = np.arange(1, 15)\n\n# Store training and test errors\ntrain_errors = []\ntest_errors = []\n\n# Generate test set (unseen data)\nx_test = np.linspace(-3, 3, 100).reshape(-1, 1)\ny_test = 2 * x_test.squeeze() ** 2 - 3 * x_test.squeeze() + 2  # True function without noise\n\n# Loop over polynomial degrees\nfor d in degrees:\n    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n\n    # Train the model\n    model.fit(x, y)\n\n    # Compute training error\n    y_train_pred = model.predict(x)\n    train_error = mean_squared_error(y, y_train_pred)\n    train_errors.append(train_error)\n\n    # Compute test error\n    y_test_pred = model.predict(x_test)\n    test_error = mean_squared_error(y_test, y_test_pred)\n    test_errors.append(test_error)\n\n# Plot the U-shaped Bias-Variance Tradeoff Curve\nplt.figure(figsize=(8, 5))\nplt.plot(degrees, train_errors, label=\"Training Error\", marker=\"o\", linestyle=\"--\")\nplt.plot(degrees, test_errors, label=\"Test Error\", marker=\"s\", linestyle=\"-\")\nplt.xlabel(\"Model Complexity (Polynomial Degree)\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Bias-Variance Tradeoff in Linear Regression\")\nplt.legend()\nplt.ylim(0, max(test_errors) * 1.2)  # Adjust y-axis limit\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Generate data\nn = 30\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Prepare cross-validation\nk = 5  # Number of CV folds\nkf = KFold(n_splits=k, shuffle=True, random_state=1)\n\n# Generate smooth x values for plotting fitted models\nxx = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n\n# Cross-validation loop\nfor i, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n    # Training and test sets\n    x_train, x_test = x[train_idx], x[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n    # Train linear model\n    lm_1 = LinearRegression()\n    lm_1.fit(x_train, y_train)\n\n    # Train 10th-order polynomial model\n    poly = PolynomialFeatures(degree=10)\n    X_train_poly = poly.fit_transform(x_train)\n    X_test_poly = poly.transform(x_test)\n    XX_poly = poly.transform(xx)\n\n    lm_10 = LinearRegression()\n    lm_10.fit(X_train_poly, y_train)\n\n    # Predictions\n    y_pred_1 = lm_1.predict(x_test)\n    y_pred_10 = lm_10.predict(X_test_poly)\n\n    # Compute test errors (MSE)\n    test_err_1 = np.mean((y_test - y_pred_1) ** 2)\n    test_err_10 = np.mean((y_test - y_pred_10) ** 2)\n\n    # Create figure and axes for side-by-side plots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n\n    # Define colors for points (red for test fold, gray for training data)\n    colors = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')\n\n    # Plot linear model (1st order)\n    axes[0].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n    axes[0].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n    axes[0].plot(xx, lm_1.predict(xx), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"1st Order Fit\")\n    axes[0].set_title(f\"Fold {i} - Linear Model\")\n    axes[0].legend()\n    axes[0].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_1:.3f}\", fontsize=12, color=\"blue\")\n\n    # Plot 10th order polynomial model\n    axes[1].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n    axes[1].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n    axes[1].plot(xx, lm_10.predict(XX_poly), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"10th Order Fit\")\n    axes[1].set_title(f\"Fold {i} - 10th Order Model\")\n    axes[1].legend()\n    axes[1].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_10:.3f}\", fontsize=12, color=\"blue\")\n\n    # Ensure consistent axes across subplots\n    axes[0].set_xlim(min(x), max(x))\n    axes[1].set_xlim(min(x), max(x))\n    axes[0].set_ylim(min(y) - 2, max(y) + 2)\n    axes[1].set_ylim(min(y) - 2, max(y) + 2)\n\n    # plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Generate data\nn = 30\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Prepare cross-validation\nk = 5  # Number of CV folds\nkf = KFold(n_splits=k, shuffle=True, random_state=1)\n\n# Generate smooth x values for plotting fitted models\nxx = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n\n# Store the errors for cross-validation\ncv_errors_1 = []\ncv_errors_10 = []\n\n# Cross-validation loop\nfor i, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n    # Training and test sets\n    x_train, x_test = x[train_idx], x[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n    # Train linear model\n    lm_1 = LinearRegression()\n    lm_1.fit(x_train, y_train)\n\n    # Train 10th-order polynomial model\n    poly = PolynomialFeatures(degree=10)\n    X_train_poly = poly.fit_transform(x_train)\n    X_test_poly = poly.transform(x_test)\n    XX_poly = poly.transform(xx)\n\n    lm_10 = LinearRegression()\n    lm_10.fit(X_train_poly, y_train)\n\n    # Predictions\n    y_pred_1 = lm_1.predict(x_test)\n    y_pred_10 = lm_10.predict(X_test_poly)\n\n    # Compute test errors (MSE)\n    test_err_1 = np.mean((y_test - y_pred_1) ** 2)\n    test_err_10 = np.mean((y_test - y_pred_10) ** 2)\n\n    # Store errors for cross-validation MSE\n    cv_errors_1.append(test_err_1)\n    cv_errors_10.append(test_err_10)\n\n    # Create figure and axes for side-by-side plots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n\n    # Define colors for points (red for test fold, gray for training data)\n    colors = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')\n\n    # Plot linear model (1st order)\n    axes[0].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n    axes[0].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n    axes[0].plot(xx, lm_1.predict(xx), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"1st Order Fit\")\n    axes[0].set_title(f\"Fold {i} - Linear Model\")\n    axes[0].legend()\n    axes[0].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_1:.3f}\", fontsize=12, color=\"blue\")\n\n    # Plot 10th order polynomial model\n    axes[1].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n    axes[1].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n    axes[1].plot(xx, lm_10.predict(XX_poly), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"10th Order Fit\")\n    axes[1].set_title(f\"Fold {i} - 10th Order Model\")\n    axes[1].legend()\n    axes[1].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_10:.3f}\", fontsize=12, color=\"blue\")\n\n    # Ensure consistent axes across subplots\n    axes[0].set_xlim(min(x), max(x))\n    axes[1].set_xlim(min(x), max(x))\n    axes[0].set_ylim(min(y) - 2, max(y) + 2)\n    axes[1].set_ylim(min(y) - 2, max(y) + 2)\n\n    # Show plots for each fold\n    plt.tight_layout()\n    plt.show()\n\n# Calculate and print the Cross-Validation Mean Squared Errors\ncv_mse_1 = np.mean(cv_errors_1)\ncv_mse_10 = np.mean(cv_errors_10)\nprint(f\"Cross-Validation MSE for Linear Model: {cv_mse_1:.3f}\")\nprint(f\"Cross-Validation MSE for 10th Order Model: {cv_mse_10:.3f}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCross-Validation MSE for Linear Model: 2.903\nCross-Validation MSE for 10th Order Model: 353.434"
  },
  {
    "objectID": "lect2.html",
    "href": "lect2.html",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lecture, students should be able to:\n\nExplain why splitting the training data to create a validation set is important.\n\nUnderstand the concept of cross-validation and its role in model training.\n\nUse Python to estimate out-of-sample prediction error using Leave-One-Out Cross-validation (LOOCV).\n\nUse Python to estimate out-of-sample prediction error using k-Fold Cross-validation.\n\nCompare and contrast the advantages and disadvantages of LOOCV and k-Fold Cross-validation in regression.\n\nExplain the Bias-Variance Trade-Off in the context of k-Fold Cross-validation for regression models.\n\n\n\n\nIf we use all of our training data to fit our regression models, estimating prediction error with in-sample (training) error is essentially the best thing we can do. To avoid this, a common approach to evaluate model performance is to split the available data into a (smaller) training set and a validation set. We can then fit our model using only the smaller training set and evaluate its performance by predicting the responses for the validation set. This allows us to simulate the model’s prediction error on unseen data while still having access to the true responses outside the test set.\n\n\n\nOne way to split the training data to get a validation set is to hold out one of the data points. We can then train the model using the remaining \\((n-1)\\) data points and estimate the prediction error by\n\\[(y_n - \\hat{f}^{-n}(x_n))^2,\\]\nwhere \\(\\hat{f}^{-n}\\) denotes the regression function estimate using all but the data point \\((y_n, x_n)\\).\nHowever, this estimate highly depends on the chosen point and can have very high variance.\n\n\n\nActivity: In-class discussion\n\n\n\nIs \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) a reasonable estimate for the out-of-sample prediction error? Discuss with your neighbors and report back.\n\n\n\n\n\n\n\n\nOne way to modify the above approach to reduce the variability is to repeat the process for every single point and look at the average of the error estimates, i.e., estimate the prediction error by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}^{-i}(x_i))^2.\\]\nThis approach is called leave-one-out cross-validation (LOOCV). Notice that we are essentially estimating \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) empirically here. Also, since we are repeating the training process \\(n\\) times, the variance of our prediction is highly reduced. However, since the size of our dataset during the training phase is roughly the same as the original training set, LOOCV increases the computational load by a factor of \\(n\\).\n\n\n\nA further modification to leave-one-out cross-validation is to, instead of holding out 1 data point at a time and repeating this process \\(n\\) times, split the training data into \\(k\\) folds \\(F_1, \\dots, F_k\\), fit our regression model using data from \\((k-1)\\) folds, and calculate the error for the \\(k\\)th fold using:\n\\[\\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2,\\]\nwhere \\(n_k = |F_k|\\), and \\(\\hat{f}^{-(k)}\\) denotes the regression function trained using all folds but the \\(k\\)th one. We can then repeat this process \\(k\\) times to estimate our prediction error by averaging over the errors calculated for each fold:\n\\[\\frac{1}{k} \\sum_{k=1}^k \\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2.\\]\nThis method is called \\(K\\)-fold cross-validation.\nExample: An illustration of 5-fold cross-validation.\n\n\n\n\n\n\nsource: https://python.datasciencebook.ca/classification2.html\n\n\n\n\n\nIn cross-validation, when \\(k &lt; n\\), besides the computational advantage of \\(K\\)-fold over LOOCV, there is another important benefit, particularly in the context of estimating the prediction error. Remember that in Lecture 1, we discussed the bias-variance trade-off and how the prediction error consists of both a bias and a variance term.\nSince each iteration of LOOCV uses almost the same data, the outputs are highly correlated, which leads to higher variance compared to \\(K\\)k-fold cross-validation. In \\(K\\)k-fold cross-validation, each fold is relatively less correlated with the others, resulting in a smaller variance. However, note that as we increase the number of folds toward LOOCV (note that when \\(K = n\\), we have LOOCV), the bias term increases. Therefore, a trade-off exists between bias and variance.\nIn practice, using \\(K=5\\) or \\(K=10\\) often provides a moderate balance between bias and variance.\n\n\n\n\n\nWe can run LOOCV in Python by using the function LeaveOneOut from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize LOOCV\nloo = LeaveOneOut()\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform LOOCV\nfor train_index, test_index in loo.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the left-out test point\n    y_pred = model.predict(x_test)\n\n    # Compute squared error\n    error = (y_test - y_pred) ** 2\n    errors.append(error)\n\n# Compute mean squared error\nloocv_mse = np.mean(errors)\n\n# Print results\nprint(f\"LOOCV Mean Squared Error: {loocv_mse:.4f}\")\n\nLOOCV Mean Squared Error: 3.3991\n\n\n\n\n\nWe can run LOOCV in Python by using the function KFold from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize 10-Fold Cross-validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform 10-Fold Cross-validation\nfor train_index, test_index in kf.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute mean squared error for this fold\n    error = mean_squared_error(y_test, y_pred)\n    errors.append(error)\n    # print(error)\n\n# Compute mean squared error across all folds\nkfold_mse = np.mean(errors)\n\n# Print results\nprint(f\"10-Fold Cross-validation Mean Squared Error: {kfold_mse:.4f}\")\n\n10-Fold Cross-validation Mean Squared Error: 3.3532\n\n\n\n\n\nActivity: In-class live-coding exercise\n\n\n\nUse the following code to generate plots for \\(K\\)-fold cross-validation. Test at least 5 different values for the pair (\\(K\\), \\(n\\)), and 5 different seeds and report back your observations.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Generate data\nn = 30\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Prepare cross-validation\nk = 5  # Number of CV folds\nkf = KFold(n_splits=k, shuffle=True, random_state=1)\n\n# Generate smooth x values for plotting fitted models\nxx = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n\n# Cross-validation loop\nfor i, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n   # Training and test sets\n   x_train, x_test = x[train_idx], x[test_idx]\n   y_train, y_test = y[train_idx], y[test_idx]\n\n   # Train linear model\n   lm_1 = LinearRegression()\n   lm_1.fit(x_train, y_train)\n\n   # Train 10th-order polynomial model\n   poly = PolynomialFeatures(degree=10)\n   X_train_poly = poly.fit_transform(x_train)\n   X_test_poly = poly.transform(x_test)\n   XX_poly = poly.transform(xx)\n\n   lm_10 = LinearRegression()\n   lm_10.fit(X_train_poly, y_train)\n\n   # Predictions\n   y_pred_1 = lm_1.predict(x_test)\n   y_pred_10 = lm_10.predict(X_test_poly)\n\n   # Compute test errors (MSE)\n   test_err_1 = np.mean((y_test - y_pred_1) ** 2)\n   test_err_10 = np.mean((y_test - y_pred_10) ** 2)\n\n   # Create figure and axes for side-by-side plots\n   fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n\n   # Define colors for points (red for test fold, gray for training data)\n   colors = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')\n\n   # Plot linear model (1st order)\n   axes[0].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n   axes[0].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n   axes[0].plot(xx, lm_1.predict(xx), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"1st Order Fit\")\n   axes[0].set_title(f\"Fold {i} - Linear Model\")\n   axes[0].legend()\n   axes[0].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_1:.3f}\", fontsize=12, color=\"blue\")\n\n   # Plot 10th order polynomial model\n   axes[1].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n   axes[1].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n   axes[1].plot(xx, lm_10.predict(XX_poly), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"10th Order Fit\")\n   axes[1].set_title(f\"Fold {i} - 10th Order Model\")\n   axes[1].legend()\n   axes[1].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_10:.3f}\", fontsize=12, color=\"blue\")\n\n   # Ensure consistent axes across subplots\n   axes[0].set_xlim(min(x), max(x))\n   axes[1].set_xlim(min(x), max(x))\n   axes[0].set_ylim(min(y) - 2, max(y) + 2)\n   axes[1].set_ylim(min(y) - 2, max(y) + 2)\n\n   plt.show()\n\n\nExample of an output:",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#learning-outcomes",
    "href": "lect2.html#learning-outcomes",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "By the end of this lecture, students should be able to:\n\nExplain why splitting the training data to create a validation set is important.\n\nUnderstand the concept of cross-validation and its role in model training.\n\nUse Python to estimate out-of-sample prediction error using Leave-One-Out Cross-validation (LOOCV).\n\nUse Python to estimate out-of-sample prediction error using k-Fold Cross-validation.\n\nCompare and contrast the advantages and disadvantages of LOOCV and k-Fold Cross-validation in regression.\n\nExplain the Bias-Variance Trade-Off in the context of k-Fold Cross-validation for regression models.",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#splitting-the-training-data",
    "href": "lect2.html#splitting-the-training-data",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "If we use all of our training data to fit our regression models, estimating prediction error with in-sample (training) error is essentially the best thing we can do. To avoid this, a common approach to evaluate model performance is to split the available data into a (smaller) training set and a validation set. We can then fit our model using only the smaller training set and evaluate its performance by predicting the responses for the validation set. This allows us to simulate the model’s prediction error on unseen data while still having access to the true responses outside the test set.",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#a-single-held-out-point",
    "href": "lect2.html#a-single-held-out-point",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "One way to split the training data to get a validation set is to hold out one of the data points. We can then train the model using the remaining \\((n-1)\\) data points and estimate the prediction error by\n\\[(y_n - \\hat{f}^{-n}(x_n))^2,\\]\nwhere \\(\\hat{f}^{-n}\\) denotes the regression function estimate using all but the data point \\((y_n, x_n)\\).\nHowever, this estimate highly depends on the chosen point and can have very high variance.\n\n\n\nActivity: In-class discussion\n\n\n\nIs \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) a reasonable estimate for the out-of-sample prediction error? Discuss with your neighbors and report back.",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#cross-validation",
    "href": "lect2.html#cross-validation",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "One way to modify the above approach to reduce the variability is to repeat the process for every single point and look at the average of the error estimates, i.e., estimate the prediction error by\n\\[\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}^{-i}(x_i))^2.\\]\nThis approach is called leave-one-out cross-validation (LOOCV). Notice that we are essentially estimating \\(\\mathbb{E}[(y_n - \\hat{f}^{-n}(x_n))^2]\\) empirically here. Also, since we are repeating the training process \\(n\\) times, the variance of our prediction is highly reduced. However, since the size of our dataset during the training phase is roughly the same as the original training set, LOOCV increases the computational load by a factor of \\(n\\).\n\n\n\nA further modification to leave-one-out cross-validation is to, instead of holding out 1 data point at a time and repeating this process \\(n\\) times, split the training data into \\(k\\) folds \\(F_1, \\dots, F_k\\), fit our regression model using data from \\((k-1)\\) folds, and calculate the error for the \\(k\\)th fold using:\n\\[\\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2,\\]\nwhere \\(n_k = |F_k|\\), and \\(\\hat{f}^{-(k)}\\) denotes the regression function trained using all folds but the \\(k\\)th one. We can then repeat this process \\(k\\) times to estimate our prediction error by averaging over the errors calculated for each fold:\n\\[\\frac{1}{k} \\sum_{k=1}^k \\frac{1}{n_k} \\sum_{i \\in F_k} (y_i - \\hat{f}^{-(k)}(x_i))^2.\\]\nThis method is called \\(K\\)-fold cross-validation.\nExample: An illustration of 5-fold cross-validation.\n\n\n\n\n\n\nsource: https://python.datasciencebook.ca/classification2.html",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#bias-variance-trade-off-for-k-fold-cross-validation",
    "href": "lect2.html#bias-variance-trade-off-for-k-fold-cross-validation",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "In cross-validation, when \\(k &lt; n\\), besides the computational advantage of \\(K\\)-fold over LOOCV, there is another important benefit, particularly in the context of estimating the prediction error. Remember that in Lecture 1, we discussed the bias-variance trade-off and how the prediction error consists of both a bias and a variance term.\nSince each iteration of LOOCV uses almost the same data, the outputs are highly correlated, which leads to higher variance compared to \\(K\\)k-fold cross-validation. In \\(K\\)k-fold cross-validation, each fold is relatively less correlated with the others, resulting in a smaller variance. However, note that as we increase the number of folds toward LOOCV (note that when \\(K = n\\), we have LOOCV), the bias term increases. Therefore, a trade-off exists between bias and variance.\nIn practice, using \\(K=5\\) or \\(K=10\\) often provides a moderate balance between bias and variance.",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "lect2.html#python-implementation",
    "href": "lect2.html#python-implementation",
    "title": "Estimation of out-of-sample prediction error in regression",
    "section": "",
    "text": "We can run LOOCV in Python by using the function LeaveOneOut from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize LOOCV\nloo = LeaveOneOut()\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform LOOCV\nfor train_index, test_index in loo.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the left-out test point\n    y_pred = model.predict(x_test)\n\n    # Compute squared error\n    error = (y_test - y_pred) ** 2\n    errors.append(error)\n\n# Compute mean squared error\nloocv_mse = np.mean(errors)\n\n# Print results\nprint(f\"LOOCV Mean Squared Error: {loocv_mse:.4f}\")\n\nLOOCV Mean Squared Error: 3.3991\n\n\n\n\n\nWe can run LOOCV in Python by using the function KFold from the sklearn library. Here’s an example to illusterate how to do so:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define number of samples\nn = 100 \n\n# Generate training data\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise\n\n# Initialize 10-Fold Cross-validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel = LinearRegression()\n\n# Store errors\nerrors = []\n\n# Perform 10-Fold Cross-validation\nfor train_index, test_index in kf.split(x):\n    # Split the data\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute mean squared error for this fold\n    error = mean_squared_error(y_test, y_pred)\n    errors.append(error)\n    # print(error)\n\n# Compute mean squared error across all folds\nkfold_mse = np.mean(errors)\n\n# Print results\nprint(f\"10-Fold Cross-validation Mean Squared Error: {kfold_mse:.4f}\")\n\n10-Fold Cross-validation Mean Squared Error: 3.3532\n\n\n\n\n\nActivity: In-class live-coding exercise\n\n\n\nUse the following code to generate plots for \\(K\\)-fold cross-validation. Test at least 5 different values for the pair (\\(K\\), \\(n\\)), and 5 different seeds and report back your observations.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\n# Set seed for reproducibility\nnp.random.seed(1)\n\n# Generate data\nn = 30\nx = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\ny = 2 * x.ravel() + 2 * np.random.randn(n)\n\n# Prepare cross-validation\nk = 5  # Number of CV folds\nkf = KFold(n_splits=k, shuffle=True, random_state=1)\n\n# Generate smooth x values for plotting fitted models\nxx = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n\n# Cross-validation loop\nfor i, (train_idx, test_idx) in enumerate(kf.split(x), 1):\n   # Training and test sets\n   x_train, x_test = x[train_idx], x[test_idx]\n   y_train, y_test = y[train_idx], y[test_idx]\n\n   # Train linear model\n   lm_1 = LinearRegression()\n   lm_1.fit(x_train, y_train)\n\n   # Train 10th-order polynomial model\n   poly = PolynomialFeatures(degree=10)\n   X_train_poly = poly.fit_transform(x_train)\n   X_test_poly = poly.transform(x_test)\n   XX_poly = poly.transform(xx)\n\n   lm_10 = LinearRegression()\n   lm_10.fit(X_train_poly, y_train)\n\n   # Predictions\n   y_pred_1 = lm_1.predict(x_test)\n   y_pred_10 = lm_10.predict(X_test_poly)\n\n   # Compute test errors (MSE)\n   test_err_1 = np.mean((y_test - y_pred_1) ** 2)\n   test_err_10 = np.mean((y_test - y_pred_10) ** 2)\n\n   # Create figure and axes for side-by-side plots\n   fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n\n   # Define colors for points (red for test fold, gray for training data)\n   colors = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')\n\n   # Plot linear model (1st order)\n   axes[0].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n   axes[0].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n   axes[0].plot(xx, lm_1.predict(xx), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"1st Order Fit\")\n   axes[0].set_title(f\"Fold {i} - Linear Model\")\n   axes[0].legend()\n   axes[0].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_1:.3f}\", fontsize=12, color=\"blue\")\n\n   # Plot 10th order polynomial model\n   axes[1].scatter(x, y, c=colors, label=\"Other folds\", alpha=0.6)\n   axes[1].scatter(x[test_idx], y[test_idx], color='red', label=f\"Fold {i}\")\n   axes[1].plot(xx, lm_10.predict(XX_poly), linestyle=\"dashed\", linewidth=2, color=\"black\", label=\"10th Order Fit\")\n   axes[1].set_title(f\"Fold {i} - 10th Order Model\")\n   axes[1].legend()\n   axes[1].text(min(x) + 3.2, min(y) + 1, f\"Test Error: {test_err_10:.3f}\", fontsize=12, color=\"blue\")\n\n   # Ensure consistent axes across subplots\n   axes[0].set_xlim(min(x), max(x))\n   axes[1].set_xlim(min(x), max(x))\n   axes[0].set_ylim(min(y) - 2, max(y) + 2)\n   axes[1].set_ylim(min(y) - 2, max(y) + 2)\n\n   plt.show()\n\n\nExample of an output:",
    "crumbs": [
      "Home",
      "Lecture 2: Cross-validation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]