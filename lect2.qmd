---
title: "Estimation of out-of-sample prediction error in regression"
execute:
  echo: false
format: html
jupyter: quarto_env 
---

# Lecture 2: Cross-validation

## Learning outcomes:

By the end of this lecture, students should be able to:

- Explain why splitting the training data to create a **validation set** is important.  
- Understand the **concept of cross-validation** and its role in model training.  
- Use Python to estimate **out-of-sample prediction error** using **Leave-One-Out Cross-validation (LOOCV)**.  
- Use Python to estimate **out-of-sample prediction error** using **k-Fold Cross-validation**.  
- Compare and contrast the **advantages and disadvantages** of **LOOCV** and **k-Fold Cross-validation** in regression.  
- Explain the **Bias-Variance Trade-Off** in the context of **k-Fold Cross-validation** for regression models.  

## Splitting the Training Data

If we use all of our training data to fit our regression models, estimating prediction error with in-sample (training) error is essentially the best thing we can do. To avoid this, a common approach to evaluate model performance is to split the available data into a (smaller) training set and a validation set. We can then fit our model using only the smaller training set and evaluate its performance by predicting the responses for the validation set. This allows us to simulate the modelâ€™s prediction error on unseen data while still having access to the true responses outside the test set.

## A Single Held-Out Point

One way to split the training data to get a validation set is to hold out one of the data points. We can then train the model using the remaining $(n-1)$ data points and estimate the prediction error by

$$(y_n - \hat{f}^{-n}(x_n))^2,$$

where $\hat{f}^{-n}$ denotes the regression function estimate using all but the data point $(y_n, x_n)$.

However, this estimate highly depends on the chosen point and can have very high variance.
---

:::{.activity}

::::{.activity-header}
Activity: In-class discussion

::::
::::{.activity-container}

 - Is $\mathbb{E}[(y_n - \hat{f}^{-n}(x_n))^2]$ a reasonable estimate for the out-of-sample prediction error? Discuss with your neighbors and report back.

::::
:::

## Cross-validation

### Leave-One-Out Cross-validation

One way to modify the above approach to reduce the variability is to repeat the process for every single point and look at the average of the error estimates, i.e., estimate the prediction error by

$$\frac{1}{n} \sum_{i=1}^n (y_i - \hat{f}^{-i}(x_i))^2.$$

This approach is called **leave-one-out cross-validation (LOOCV)**. Notice that we are essentially estimating $\mathbb{E}[(y_n - \hat{f}^{-n}(x_n))^2]$ empirically here. Also, since we are repeating the training process $n$ times, the variance of our prediction is highly reduced. However, since the size of our dataset during the training phase is roughly the same as the original training set, LOOCV increases the computational load by a factor of $n$.

### $K$-Fold Cross-validation

A further modification to leave-one-out cross-validation is to, instead of holding out 1 data point at a time and repeating this process $n$ times, split the training data into $k$ folds $F_1, \dots, F_k$, fit our regression model using data from $(k-1)$ folds, and calculate the error for the $k$th fold using:

$$\frac{1}{n_k} \sum_{i \in F_k} (y_i - \hat{f}^{-(k)}(x_i))^2,$$

where $n_k = |F_k|$, and $\hat{f}^{-(k)}$ denotes the regression function trained using all folds but the $k$th one. We can then repeat this process $k$ times to estimate our prediction error by averaging over the errors calculated for each fold:

$$\frac{1}{k} \sum_{k=1}^k \frac{1}{n_k} \sum_{i \in F_k} (y_i - \hat{f}^{-(k)}(x_i))^2.$$

This method is called **$K$-fold cross-validation**.

Example: An illustration of 5-fold cross-validation.

![](example.png){fig-align="center" width="50%"}

:::{style="font-size: 15px;"}
source: [https://python.datasciencebook.ca/classification2.html](https://python.datasciencebook.ca/classification2.html)
:::


## Bias-Variance Trade-Off for $K$-Fold Cross-validation

In cross-validation, when $k < n$, besides the computational advantage of $K$-fold over LOOCV, there is another important benefit, particularly in the context of estimating the prediction error. Remember that in Lecture 1, we discussed the bias-variance trade-off and how the prediction error consists of both a bias and a variance term.

Since each iteration of LOOCV uses almost the same data, the outputs are highly correlated, which leads to higher variance compared to $K$k-fold cross-validation. In $K$k-fold cross-validation, each fold is relatively less correlated with the others, resulting in a smaller variance. However, note that as we increase the number of folds toward LOOCV (note that when $K = n$, we have LOOCV), the bias term increases. Therefore, a trade-off exists between bias and variance.

In practice, using $K=5$ or $K=10$  often provides a moderate balance between bias and variance.


## Python implementation 

### How to perform LOOCV in Python

We can run LOOCV in Python by using the function `LeaveOneOut` from the `sklearn` library. Here's an example to illusterate how to do so:

```{python}
#| echo: true
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import LeaveOneOut
from sklearn.metrics import mean_squared_error

# Set random seed for reproducibility
np.random.seed(42)

# Define number of samples
n = 100 

# Generate training data
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise

# Initialize LOOCV
loo = LeaveOneOut()
model = LinearRegression()

# Store errors
errors = []

# Perform LOOCV
for train_index, test_index in loo.split(x):
    # Split the data
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Train the model
    model.fit(x_train, y_train)

    # Predict on the left-out test point
    y_pred = model.predict(x_test)

    # Compute squared error
    error = (y_test - y_pred) ** 2
    errors.append(error)

# Compute mean squared error
loocv_mse = np.mean(errors)

# Print results
print(f"LOOCV Mean Squared Error: {loocv_mse:.4f}")

```

### How to perform $K$-fold cross-validation in Python

We can run LOOCV in Python by using the function `KFold` from the `sklearn` library. Here's an example to illusterate how to do so:


```{python}
#| echo: true

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error

# Set random seed for reproducibility
np.random.seed(42)

# Define number of samples
n = 100 

# Generate training data
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.squeeze() + 2 * np.random.randn(n)  # Linear relationship with noise

# Initialize 10-Fold Cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
model = LinearRegression()

# Store errors
errors = []

# Perform 10-Fold Cross-validation
for train_index, test_index in kf.split(x):
    # Split the data
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Train the model
    model.fit(x_train, y_train)

    # Predict on the test set
    y_pred = model.predict(x_test)

    # Compute mean squared error for this fold
    error = mean_squared_error(y_test, y_pred)
    errors.append(error)
    # print(error)

# Compute mean squared error across all folds
kfold_mse = np.mean(errors)

# Print results
print(f"10-Fold Cross-validation Mean Squared Error: {kfold_mse:.4f}")


```


---

:::{.activity}

::::{.activity-header}
Activity: In-class live-coding exercise

::::
::::{.activity-container}

 - Use the following code to generate  plots for $K$-fold cross-validation. Test at least 5 different values for the pair ($K$, $n$), and 5 different seeds and report back your observations.


 ```{.python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import KFold

# Set seed for reproducibility
np.random.seed(1)

# Generate data
n = 30
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.ravel() + 2 * np.random.randn(n)

# Prepare cross-validation
k = 5  # Number of CV folds
kf = KFold(n_splits=k, shuffle=True, random_state=1)

# Generate smooth x values for plotting fitted models
xx = np.linspace(min(x), max(x), 100).reshape(-1, 1)

# Cross-validation loop
for i, (train_idx, test_idx) in enumerate(kf.split(x), 1):
    # Training and test sets
    x_train, x_test = x[train_idx], x[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Train linear model
    lm_1 = LinearRegression()
    lm_1.fit(x_train, y_train)

    # Train 10th-order polynomial model
    poly = PolynomialFeatures(degree=10)
    X_train_poly = poly.fit_transform(x_train)
    X_test_poly = poly.transform(x_test)
    XX_poly = poly.transform(xx)

    lm_10 = LinearRegression()
    lm_10.fit(X_train_poly, y_train)

    # Predictions
    y_pred_1 = lm_1.predict(x_test)
    y_pred_10 = lm_10.predict(X_test_poly)

    # Compute test errors (MSE)
    test_err_1 = np.mean((y_test - y_pred_1) ** 2)
    test_err_10 = np.mean((y_test - y_pred_10) ** 2)

    # Create figure and axes for side-by-side plots
    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)

    # Define colors for points (red for test fold, gray for training data)
    colors = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')

    # Plot linear model (1st order)
    axes[0].scatter(x, y, c=colors, label="Other folds", alpha=0.6)
    axes[0].scatter(x[test_idx], y[test_idx], color='red', label=f"Fold {i}")
    axes[0].plot(xx, lm_1.predict(xx), linestyle="dashed", linewidth=2, color="black", label="1st Order Fit")
    axes[0].set_title(f"Fold {i} - Linear Model")
    axes[0].legend()
    axes[0].text(min(x) + 3.2, min(y) + 1, f"Test Error: {test_err_1:.3f}", fontsize=12, color="blue")

    # Plot 10th order polynomial model
    axes[1].scatter(x, y, c=colors, label="Other folds", alpha=0.6)
    axes[1].scatter(x[test_idx], y[test_idx], color='red', label=f"Fold {i}")
    axes[1].plot(xx, lm_10.predict(XX_poly), linestyle="dashed", linewidth=2, color="black", label="10th Order Fit")
    axes[1].set_title(f"Fold {i} - 10th Order Model")
    axes[1].legend()
    axes[1].text(min(x) + 3.2, min(y) + 1, f"Test Error: {test_err_10:.3f}", fontsize=12, color="blue")

    # Ensure consistent axes across subplots
    axes[0].set_xlim(min(x), max(x))
    axes[1].set_xlim(min(x), max(x))
    axes[0].set_ylim(min(y) - 2, max(y) + 2)
    axes[1].set_ylim(min(y) - 2, max(y) + 2)

    plt.show()
 ```

::::
:::

Example of an output:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import KFold

# Set seed for reproducibility
np.random.seed(1)

# Generate data
n = 30
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.ravel() + 2 * np.random.randn(n)

# Prepare cross-validation
k = 5  # Number of CV folds
kf = KFold(n_splits=k, shuffle=True, random_state=1)

# Generate smooth x values for plotting fitted models
xx = np.linspace(min(x), max(x), 100).reshape(-1, 1)

# Cross-validation loop
for i, (train_idx, test_idx) in enumerate(kf.split(x), 1):
    # Training and test sets
    x_train, x_test = x[train_idx], x[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Train linear model
    lm_1 = LinearRegression()
    lm_1.fit(x_train, y_train)

    # Train 10th-order polynomial model
    poly = PolynomialFeatures(degree=10)
    X_train_poly = poly.fit_transform(x_train)
    X_test_poly = poly.transform(x_test)
    XX_poly = poly.transform(xx)

    lm_10 = LinearRegression()
    lm_10.fit(X_train_poly, y_train)

    # Predictions
    y_pred_1 = lm_1.predict(x_test)
    y_pred_10 = lm_10.predict(X_test_poly)

    # Compute test errors (MSE)
    test_err_1 = np.mean((y_test - y_pred_1) ** 2)
    test_err_10 = np.mean((y_test - y_pred_10) ** 2)

    # Create figure and axes for side-by-side plots
    fig, axes = plt.subplots(1, 2, figsize=(11, 5), sharex=True, sharey=True)

    # Define colors for points (red for test fold, gray for training data)
    colors = np.where(np.isin(np.arange(n), test_idx), 'red', 'gray')

    # Plot linear model (1st order)
    axes[0].scatter(x, y, c=colors, label="Other folds", alpha=0.6)
    axes[0].scatter(x[test_idx], y[test_idx], color='red', label=f"Fold {i}")
    axes[0].plot(xx, lm_1.predict(xx), linestyle="dashed", linewidth=2, color="black", label="1st Order Fit")
    axes[0].set_title(f"Fold {i} - Linear Model")
    axes[0].legend()
    axes[0].text(min(x) + 3.2, min(y) + 1, f"Test Error: {test_err_1:.3f}", fontsize=12, color="blue")

    # Plot 10th order polynomial model
    axes[1].scatter(x, y, c=colors, label="Other folds", alpha=0.6)
    axes[1].scatter(x[test_idx], y[test_idx], color='red', label=f"Fold {i}")
    axes[1].plot(xx, lm_10.predict(XX_poly), linestyle="dashed", linewidth=2, color="black", label="10th Order Fit")
    axes[1].set_title(f"Fold {i} - 10th Order Model")
    axes[1].legend()
    axes[1].text(min(x) + 3.2, min(y) + 1, f"Test Error: {test_err_10:.3f}", fontsize=12, color="blue")

    # Ensure consistent axes across subplots
    axes[0].set_xlim(min(x), max(x))
    axes[1].set_xlim(min(x), max(x))
    axes[0].set_ylim(min(y) - 2, max(y) + 2)
    axes[1].set_ylim(min(y) - 2, max(y) + 2)

    plt.show()

```



