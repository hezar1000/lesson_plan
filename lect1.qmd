---
title: "Estimation of out-of-sample prediction error in regression
"
execute:
  echo: false
format: html
jupyter: quarto_env 
---

# Lecture 1: Basic concepts and definitions 

## Learning outcomes:
By the end of this lecture, students should be able to:

- Differentiate between **inference** and **prediction** in regression at a conceptual level.  
- Understand the reasoning behind **splitting data** into **training and test sets** for model evaluation.  
- Distinguish between **training (in-sample) error** and **test (out-of-sample) error**, and explain their significance.  
- Use Python to estimate **out-of-sample prediction error** using **training error** in linear regression.  
- Define and identify the concepts of **overfitting** and **underfitting** in predictive modeling.  
- Explain the **Bias-Variance Trade-Off** and its implications for model performance.  

## Regression

- We have $n$ data points with $p$ different predictors: $X = (X_1, X_2, ..., X_p)$ and a response $Y$, which are independent samples from some distribution.

- We assume that there is some relationship between them, which can be written as
  $$Y = f(X) + \epsilon,$$
  where $f$ is some fixed but unknown function of $X_1, ..., X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero.

- Our goal is to **estimate** $f$ via another function called $\hat{f}$.

- When $Y$ is quantitative, we have a **regression** problem (and when it is qualitative, we have a **classification** problem).

## Inference vs Prediction
### Why estimate $f$?

- **Inference**: Focuses on identifying relationships and associations between predictors and the response.

- **Prediction**: Focuses on estimating the response for newly unseen predictors.

Our focus here is on **prediction**. But how do we know how good we are at this task?

## Training/test split

When it comes to prediction, we want to work with two different datasets: **training and test (aka out-of-sample)**. The reason for this is that we would then consider a training phase, during which the training data is used to fit (or train) the model, and a testing phase, where the model's performance is evaluated using the test dataset, which it has never seen during training. This evaluation helps us determine how well the model predicts responses for new, unseen data. From now on, we are going to use $X, Y$ to denote the training data, and $X^*, Y^*$ to denote the test data. We therefore can define the prediciton task more formally by defining it as estimating the regression function $\hat{f}$ as

$$\hat{Y^*} = \hat{f}(X^*)$$ 

for an unseen $X^*$, where $X^*, Y^*$ are sampled from the same distribution as $X, Y$

## Out-of-sample prediction error in regression

For new test data $X^*, Y^*$ sampled from the same distribution as the trainign data, we can defined the following error term called **out-of-sample prediction error** or **prediction error** in regression:
$$\mathbb{E}(Y^*−\hat{Y}^*)^2 = \mathbb{E}(Y^*−\hat{f}(X^*))^2,$$
where the expectation is taken over the training data $X, Y$, and test data $X^*, Y^*$.

### Why do we care about estimating out-of-sample error?

- To **evalute** the quality of predicition in our model,
- To be able to **minimize** the error for better prediction.

Example: Imagine to you've trained a model to predict stock market. Think about how much you care about you model being accurate in the unseen future.
---

:::{.activity}

::::{.activity-header}
Activity: In-class discussion

::::
::::{.activity-container}

 - Can we ever predict $Y^*$ from $X^*$ with zero prediction error? Discuss with your neighbors and report back

::::
:::

---

## Estimating the prediction error

In practice, we cannot use the test data $X^*$ and $Y^*$ to estimate the prediction error during the training phase because:

- In many cases, there is no test data.
- Even if there is, we are not allowed to use it during the training phase.

Therefore, we cannot directly calculate the prediction error; instead, we need a method to estimate this quantity using our training data.

### How about estimating prediction error with in-sample (training) error?
In the regression setting, the most commonly-used measure  for the quality of fit is the mean squared error (MSE), given by

$$\frac{1}{n} \sum_{i=1}^n (y_i - \hat{f}(x_i))^2$$

Note that $Y= (y_1,y_2,...,y_n)$, $X = (x_1,...,x_n)$, and $x_i = (x_{i1}, x_{i2},...,x_{ip})^T$.

Can we use this MSE as an estimate for prediction error? 
  
  - This approach usually gives an optimistic estimate of the prediction error. This is not surprising because when training our model to calculate $\hat{f}$, we are minimizing this error during the process.


## Example: Linear Regression

### Reminder
In linear regression, we have:

$$f(X) = \beta_0 + \beta_1 X_1 +  \beta_2 X_2 + \dots + \beta_p X_p,$$

and our goal is to find $\beta_0, \dots , \beta_p$. The most common approach to fit this linear model is using the least squares method, which essentially minimizes the MSE of the training set.

Therefore, after computing $\hat{f}$ through the training phase, given new $X^*$, we have:

$$\hat{Y}^* = \hat{\beta_0} + \hat{\beta_1} X^*_1 + \hat{\beta_2} X^*_2 + \dots + \hat{\beta_p} X^*_p,$$

and we can estimate the prediction error using the training error:

$$\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

Note that $Y = (y_1, y_2, \dots, y_n)$ and $\hat{Y} = (\hat{y}_1, \dots, \hat{y}_n)$.

:::{.activity}

::::{.activity-header}
Activity: Training a Linear Regression Model

::::
::::{.activity-container}

 - Use the following code to generate a training and test dataset:

 ```{.python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Set seed for reproducibility
np.random.seed(2)

# Define parameters
n = 50

# Generate training data
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x + 2 * np.random.randn(n)

# Generate test data
x_s = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y_s = 2 * x_s + 2 * np.random.randn(n)

 ```
Next, we want to let's fit a Linear Regression model to this data, find the erros, and plot the result. You can use the following code to do so:

```{.python}

# Train a simple linear model
lm_1 = LinearRegression()
lm_1.fit(x, y)

# Predictions
yhat_1 = lm_1.predict(x)
y_s_hat_1 = lm_1.predict(x_s)

# Compute errors
train_err_1 = np.mean((y - yhat_1) ** 2)
test_err_1 = np.mean((y_s - y_s_hat_1) ** 2)

# Set up plotting
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

# Define axis limits
xlim = (min(np.min(x), np.min(x_s)), max(np.max(x), np.max(x_s)))
ylim = (min(np.min(y), np.min(y_s)), max(np.max(y), np.max(y_s)))

# Plot training data
axes[0].scatter(x, y, label="Training Data")
axes[0].plot(x, yhat_1, color='red', linewidth=2, label="Fitted Line")
axes[0].set_xlim(xlim)
axes[0].set_ylim(ylim)
axes[0].set_title("Training data")
axes[0].text(0, -6, f"Training error: {train_err_1:.3f}", fontsize=12)

# Plot test data
axes[1].scatter(x_s, y_s, label="Test Data")
axes[1].plot(x_s, y_s_hat_1, color='green', linewidth=2, label="Fitted Line")
axes[1].set_xlim(xlim)
axes[1].set_ylim(ylim)
axes[1].set_title("Test data")
axes[1].text(0, -6, f"Test error: {test_err_1:.3f}", fontsize=12)

# Show plots
plt.show()
```

Try that for 10 different random seeds and report back.
::::
:::

 
---

Example of an output:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Set seed for reproducibility
np.random.seed(1)

# Generate training data
n = 30
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.ravel() + 2 * np.random.randn(n)

# Generate test data
x0 = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y0 = 2 * x0.ravel() + 2 * np.random.randn(n)

# Train a simple linear model
lm_1 = LinearRegression()
lm_1.fit(x, y)

# Predictions
yhat_1 = lm_1.predict(x)
y0hat_1 = lm_1.predict(x0)

# Compute errors
train_err_1 = np.mean((y - yhat_1) ** 2)
test_err_1 = np.mean((y0 - y0hat_1) ** 2)

# Set up plotting
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

# Define axis limits
xlim = (min(np.min(x), np.min(x0)), max(np.max(x), np.max(x0)))
ylim = (min(np.min(y), np.min(y0)), max(np.max(y), np.max(y0)))

# Plot training data
axes[0].scatter(x, y, label="Training Data")
axes[0].plot(x, yhat_1, color='red', linewidth=2, label="Fitted Line")
axes[0].set_xlim(xlim)
axes[0].set_ylim(ylim)
axes[0].set_title("Training data")
axes[0].text(0, -6, f"Training error: {train_err_1:.3f}", fontsize=12)

# Plot test data
axes[1].scatter(x0, y0, label="Test Data")
axes[1].plot(x0, y0hat_1, color='green', linewidth=2, label="Fitted Line")
axes[1].set_xlim(xlim)
axes[1].set_ylim(ylim)
axes[1].set_title("Test data")
axes[1].text(0, -6, f"Test error: {test_err_1:.3f}", fontsize=12)

# Show plots
# plt.tight_layout()
plt.show()

```


### The Bias–variance tradeoff
How about if we increase the complexity of our model?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Set seed for reproducibility
np.random.seed(1)

# Generate training data
n = 30
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.ravel() + 2 * np.random.randn(n)

# Generate test data
x0 = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y0 = 2 * x0.ravel() + 2 * np.random.randn(n)

# Create a 10th-order polynomial transformation
poly = PolynomialFeatures(degree=10)
X_poly = poly.fit_transform(x)
X0_poly = poly.transform(x0)

# Train a polynomial regression model
lm_10 = LinearRegression()
lm_10.fit(X_poly, y)

# Predictions
yhat_10 = lm_10.predict(X_poly)
y0hat_10 = lm_10.predict(X0_poly)

# Compute training and test errors
train_err_10 = np.mean((y - yhat_10) ** 2)
test_err_10 = np.mean((y0 - y0hat_10) ** 2)

# Generate points for smooth curve
xx = np.linspace(min(x), max(x), 100).reshape(-1, 1)
XX_poly = poly.transform(xx)
yy_pred = lm_10.predict(XX_poly)

# Set up plotting
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

# Define axis limits
xlim = (min(np.min(x), np.min(x0)), max(np.max(x), np.max(x0)))
ylim = (min(np.min(y), np.min(y0)), max(np.max(y), np.max(y0)))

# Plot training data
axes[0].scatter(x, y, label="Training Data")
axes[0].plot(xx, yy_pred, color='red', linewidth=2, label="10th Order Fit")
axes[0].set_xlim(xlim)
axes[0].set_ylim(ylim)
axes[0].set_title("Training data")
axes[0].text(0, -6, f"Training error: {train_err_10:.3f}", fontsize=12)

# Plot test data
axes[1].scatter(x0, y0, label="Test Data")
axes[1].plot(xx, yy_pred, color='green', linewidth=2, label="10th Order Fit")
axes[1].set_xlim(xlim)
axes[1].set_ylim(ylim)
axes[1].set_title("Test data")
axes[1].text(0, -6, f"Test error: {test_err_10:.3f}", fontsize=12)

# Show plots
# plt.tight_layout()
plt.show()

```

Interestingly, it can be shown that the **prediction error** follows the equation:

$$
\mathbb{E} \big( Y^* − \hat{f}(X^*) \big)^2 = \text{Var}(\hat{f}(X^*)) + \big[ \text{Bias}(\hat{f}(X^*)) \big]^2 + \text{Var}(\epsilon)
$$

In this equation, $Var(\epsilon)$ represents the **irreducible error**, which is independent of the data. This means that no matter how good our model is, the **prediction error** can never be lower than this variance.

However, the prediction error also consists of a **bias** term and a **variance** term, both of which depend on the data. The **variance** measures how much $\hat{f}(X^*)$ would change if we used a different training dataset. If variance is high, small changes in the training data result in large changes in $\hat{f}$, leading to **overfitting**.

  - Extreme example of overfitting: A curve that goes through every training datapoint 

The **bias** term represents the error introduced by our choice of model. If we use a very simple model, the bias might be high because it is unlikely that the true data-generating process follows such a simplistic structure. This results in **underfitting**.

  - Extreme example of overfitting: A horizontal line

As a general rule, **more flexible models** tend to have **higher variance** but **lower bias**, while **simpler models** have **higher bias** but **lower variance**. This fundamental trade-off between bias and variance is known as the **bias-variance trade-off**.

In practice, as mentioned, we often cannot explicitly compute the prediction error, bias, or variance, so, we should always keep the **bias-variance trade-off** in mind when choosing and evaluating models.

Example: bias-variance trade-off in a regression modes used to fit data with Quadratic relationship 

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic dataset
n = 30  # Number of data points
x = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)
y = 2 * x.squeeze() ** 2 - 3 * x.squeeze() + 2 + np.random.randn(n) * 2  # Quadratic relationship with noise

# Define the range of model complexities (polynomial degrees)
degrees = np.arange(1, 14)

# Store training and test errors
train_errors = []
test_errors = []

# Generate test set (unseen data)
x_test = np.linspace(-3, 3, 100).reshape(-1, 1)
y_test = 2 * x_test.squeeze() ** 2 - 3 * x_test.squeeze() + 2  # True function without noise

# Loop over polynomial degrees
for d in degrees:
    model = make_pipeline(PolynomialFeatures(d), LinearRegression())

    # Train the model
    model.fit(x, y)

    # Compute training error
    y_train_pred = model.predict(x)
    train_error = mean_squared_error(y, y_train_pred)
    train_errors.append(train_error)

    # Compute test error
    y_test_pred = model.predict(x_test)
    test_error = mean_squared_error(y_test, y_test_pred)
    test_errors.append(test_error)

# Plot the U-shaped Bias-Variance Tradeoff Curve
plt.figure(figsize=(8, 5))
plt.plot(degrees, train_errors, label="Training Error", marker="o", linestyle="--")
plt.plot(degrees, test_errors, label="Test Error", marker="s", linestyle="-")
plt.xlabel("Model Complexity (Polynomial Degree)")
plt.ylabel("Mean Squared Error")
plt.title("Bias-Variance Tradeoff in Linear Regression")
plt.legend()
plt.ylim(0, max(test_errors) * 1.2)  # Adjust y-axis limit
plt.grid()
plt.show()

```
